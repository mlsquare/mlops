[
  {
    "objectID": "notebooks/Sample-Hardness.html",
    "href": "notebooks/Sample-Hardness.html",
    "title": "Sample Hardness",
    "section": "",
    "text": "RMD\nLet us compute the Mahalanobis Distance (MD) and Relative Mahalanobis Distance (RMD) to flag RMD score\nRef A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection. In the paper, while computing the foreground MD, while the means are specific to each class, the covariance is common to all classes. The background model to compute the MD assumes a common mean and covariance. You compute the difference between the foreground MD and the background MD, and take the minimum.\nBut it is must simpler to implement (using sklearn covariance APIs) MD and RMD with class specific covariance. We will implement this slight modification of RMD for a two class problem.\n# let us compute RMD for this two features\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\nX1 = X_train[y_train==0,:]\nprint('1st group', X1.shape)\nX2 = X_train[y_train==1,:]\nprint('1st group', X2.shape)\n\n# class-specific covariances\nSig1 = MinCovDet().fit(X1)\nSig2 = MinCovDet().fit(X2)\n# covariance for the entire data, dropping the class labels\nSig = MinCovDet().fit(X_train)\n\n# sklearn has mahalanobis functions. we don't have to implement\nm1 = Sig1.mahalanobis(X_train)\nm2 = Sig2.mahalanobis(X_train)\nm = Sig.mahalanobis(X_train)\n\nrmd_1 = (m1-m)\nrmd_2 = (m2-m)\n\n\n# take the min of m1, m2 for each record, subtract m. will take exp to keep them positive.\nconf_rmd  = np.exp(-np.minimum(rmd_1, rmd_2))\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,s=conf_rmd)\n\n1st group (40, 2)\n1st group (40, 2)",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sample Hardness</span>"
    ]
  },
  {
    "objectID": "notebooks/Sample-Hardness.html#margins",
    "href": "notebooks/Sample-Hardness.html#margins",
    "title": "Sample Hardness",
    "section": "Margins",
    "text": "Margins\nGiven some representation of the data (or embedding), we can use very well known ML techniques to come up similar statistics like RMD. For example, we can fit an SVM, and calculate the margins for each instance. Not only we solve the prediciton problem, we can get secondary statistics, which are useful in determining the difficulty of the sample to the model.\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn import svm\n\nclf = svm.SVC(kernel=\"linear\", C=1000)\nclf.fit(X_train, y_train)\n\ny_test = clf.predict(X_test)\nprint('on test set')\nprint(classification_report(y_train, yh_train, target_names=['versicolor','virginica']))\n\nprint('on test set')\nprint(classification_report(y_test, yh_test, target_names=['versicolor','virginica']))\n\non test set\n              precision    recall  f1-score   support\n\n  versicolor       0.69      0.72      0.71        40\n   virginica       0.71      0.68      0.69        40\n\n    accuracy                           0.70        80\n   macro avg       0.70      0.70      0.70        80\nweighted avg       0.70      0.70      0.70        80\n\non test set\n              precision    recall  f1-score   support\n\n  versicolor       1.00      0.85      0.92        13\n   virginica       0.78      1.00      0.88         7\n\n    accuracy                           0.90        20\n   macro avg       0.89      0.92      0.90        20\nweighted avg       0.92      0.90      0.90        20\n\n\n\n\n# calculate the margin of all data points in the training set, already available in sklearn\nconf = clf.decision_function(X_train)\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,s=50*np.abs(conf))\n\n\n\n\n\n\n\n\n\n# plot the decision function\n\nax = plt.gca()\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X_train,\n    plot_method=\"contour\",\n    colors=\"k\",\n    levels=[-1, 0, 1],\n    alpha=0.5,\n    linestyles=[\"--\", \"-\", \"--\"],\n    ax=ax,\n)\n# plot support vectors\nax.scatter(\n    clf.support_vectors_[:, 0],\n    clf.support_vectors_[:, 1],\n    s=100,\n    linewidth=1,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n)\nax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,s=50*np.abs(conf))\nplt.show()\n\nprint('Train size', len(X_train))\nprint('# of support vectors', len(clf.support_vectors_))\n\n\n\n\n\n\n\n\nTrain size 80\n# of support vectors 56\n\n\nAt inference time, we can flag instances with low confidence. A simple heuristic to flag us, the confidence score has to be greater than the confidence of the suppost vectors.\n\n# get the smallest confidence that is not of a support vector\nsv = clf.support_\n# get the conf of those support vectors\nconf_sv = clf.decision_function(X_train[sv,:])\n\n\n# get conf of all points in the train set\nconf = clf.decision_function(X_train)\nprint('conf',conf.shape)\n\n# get the mix conf of SVs from the training data\nthresh = np.max(conf_sv)\n\nprint('max conf of support vectors is: ', thresh)\n\nconf (80,)\nmax conf of support vectors is:  1.2440925839422636\n\n\nEither we can remove the points with low confidence and re-train the model, or pass a sample weight based on the confidence, and retrain the model.\n\n# At inference time, flag test points as low conf or high conf\nconf_test = clf.decision_function(X_test)\n\nind_high_conf = np.where(conf_test &gt; thresh)\nprint('Test points that with high confidence', ind_high_conf[0].tolist())\n\nind_low_conf = np.where(conf_test &lt;= thresh)\nprint('Test points that with low confidence', ind_low_conf[0].tolist())\n\nTest points that with high confidence [13]\nTest points that with low confidence [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19]\n\n\nWe can look at the accuracy of test data on high conf and low conf data points on train set, as we seem to have not many points in the test set.\n\nconf_train = clf.decision_function(X_train)\nind_high_conf = np.where(conf_train &gt; thresh)[0].tolist()\n\nyh_high_conf = clf.predict(X_train[ind_high_conf,:])\nprint('on train set: high conf')\nprint(classification_report(y_train[ind_high_conf], yh_high_conf))\n\nind_low_conf = np.where(conf_train &lt;= thresh)[0].tolist()\n\nyh_low_conf = clf.predict(X_train[ind_low_conf,:])\nprint('on train set: low conf')\nprint(classification_report(y_train[ind_low_conf], yh_low_conf))\n\non train set: high conf\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00        10\n\n    accuracy                           1.00        10\n   macro avg       1.00      1.00      1.00        10\nweighted avg       1.00      1.00      1.00        10\n\non train set: low conf\n              precision    recall  f1-score   support\n\n           0       0.64      0.70      0.67        40\n           1       0.54      0.47      0.50        30\n\n    accuracy                           0.60        70\n   macro avg       0.59      0.58      0.58        70\nweighted avg       0.59      0.60      0.60        70\n\n\n\nWe achieve perfect accuracy on the train set in which all samples have high confidence. And accuracy is around ~ 60% on the samples with low confidence. This demonstrates an important aspect – not all samples will have equal degree of confidence, and if there is a way to flag them, and deal with in the downstream task, we can bring reliability into the system.\nWe have chosen the threshold based on some intuition that, typically support vectors will be closed to the separating hyper plans and will exactly sit on the hyperplanes. So, if we choose points whose are farther from the support vectors, they should be farther away from the decision boundary and hence easy to classify.\nBut this way of choosing the thresholds does not give any statistical guarantees. One to has to choose the threshold via some cross-validation procedure. Later, we will see conformalization techniques which address this issue.\nOne way to incorporate the confidence or sample easiness into training procedure is to, remove all difficult examples and retrain the model. Or convert the RMD or other types scores into weights and use a weighted loss, instead.\nThis paper Learning Sample Difficulty from Pre-trained Models for Reliable Prediction uses a score based on RMD to reweigh the samples in loss function.\n\n# create weight from conf, and fit a logistic regression with weighed samples\nweights = np.exp(0.5*conf)\nscaler = np.max(weights)\nweights = weights/scaler\nplt.plot(weights)\nplt.show()\nplt.plot(conf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# fit a logistic model with sampled weights\nweighed_model = LogisticRegression(random_state=0).fit(X_train, y_train, sample_weight=weights)\nyh_test = weighed_model.predict(X_test)\n\n# get conf from margins of svm\nconf_test = clf.decision_function(X_test)\ntest_weights = np.exp(conf_test)/scaler\n\nprint('on test set w/o weights')\nprint(classification_report(y_test, yh_test))\n\nprint('on test set with weights')\nprint(classification_report(y_test, yh_test, sample_weight = test_weights ))\n\non test set w/o weights\n              precision    recall  f1-score   support\n\n           0       1.00      0.62      0.76        13\n           1       0.58      1.00      0.74         7\n\n    accuracy                           0.75        20\n   macro avg       0.79      0.81      0.75        20\nweighted avg       0.85      0.75      0.75        20\n\non test set with weights\n              precision    recall  f1-score   support\n\n           0       1.00      0.41      0.58 1.5365777483728735\n           1       0.78      1.00      0.88 3.298054168177313\n\n    accuracy                           0.81 4.834631916550187\n   macro avg       0.89      0.70      0.73 4.834631916550187\nweighted avg       0.85      0.81      0.78 4.834631916550187\n\n\n\nOverall accuracy improves. Based on our earlier observations, we can predict class 1 much better than class 0. Interestingly, this can also be interpreted as a different form of regularization. Typically, one would place a constraint on the norm of the parameters, implying, one is enforcing smoothness constraints on the functional space. Here, by reweighting the loss, the learning algorithm gives less importance is difficulty samples, there by, the function to be fit, need to do lot of hard work (i.e very complex function) but a simpler function (meaning smooth function) would suffice. So, while the goal is same (smooth function), the way one goes about can be different. The path of regularization, to a large extent, is a brute-force approach, but reweighting one exactly knowns what is the influence of each example in the training.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sample Hardness</span>"
    ]
  },
  {
    "objectID": "notebooks/Sample-Fitness.html",
    "href": "notebooks/Sample-Fitness.html",
    "title": "Sample Fitness",
    "section": "",
    "text": "We will look at dataset difficulty and sample perplexity using Iris data.\nIris is a relative easy dataset. We will pick two features, and pick versicolor and virginica lables as there seems to be some overalp in the feature space. We will look at this two dimensional data from many angles and see what can we learn about\nSee Sample Hardness notebook for prior work. We will build from there.\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\niris = load_iris()\ny = iris.target\nind = (y==1) | (y==2)\nX = iris.data[ind, 0:2]\ny = y[ind]-1\n\nWe will build a simple logistic model and calculate the deviance between two models given data. This idea is intimately tied to likelihood ratio tests (LRT), Bayes Factors.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=128)\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n\nmodel = sm.Logit(y_train, X_train).fit()\nprint(model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.667806\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                   80\nModel:                          Logit   Df Residuals:                       78\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 15 Sep 2024   Pseudo R-squ.:                 0.03482\nTime:                        17:04:38   Log-Likelihood:                -53.424\nconverged:                       True   LL-Null:                       -55.352\nCovariance Type:            nonrobust   LLR p-value:                   0.04961\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.6830      0.377      1.811      0.070      -0.056       1.422\nx2            -1.4199      0.815     -1.742      0.082      -3.018       0.178\n==============================================================================\n\n\n\nLikelihood\nThe model summary contains many useful statistics like Log-Likelihood, LL-Null and LLR p-value. Let us explore what these different objects.\nThe generative model for Logistic regression can be written like as follows:\n\\[y_i \\sim Binomial(1,\\pi_i) \\\\\n\\log \\left(\\frac{\\pi_i}{1-\\pi_i} \\right) = x_i^T\\beta\\] where \\(y_i \\in {0,1}\\) is the binary response drawn from a Bernoulli trial with \\(P(y_i=1)=\\pi_i\\), \\(x_i\\) is a $p $ vector of input features, \\(\\beta\\) is a vector of \\(p \\times 1\\) coefficients (weights).\nThe likelihood of \\(n\\) examples (from a training set) can be written as: \\[\\ell(\\beta, D) = \\sum_{i=1}^{n} y_i \\log(\\pi_i) + (1-y_i) \\log(1-\\pi_i)\\] where \\(D = \\{x_i, y_i\\}_{i=1}^{n}\\) represents all the data available to fit (train) the model, and \\(\\pi_i\\) is as defined earlier.\nTypically, log likelihood \\(\\log(\\ell)\\) is reported. One can see that, cross-entropy is the negative log-likelihood for this problem. By following this procedure, we can come up new loss functions - define the loss as the negative log-likelihood.\n\n# log likelihood \nprint('log likelihood', model.llf)\n\nlog likelihood -53.424489860304064\n\n\n\n\nAIC\nAIC is defined as \\(2p - 2 \\ln(\\hat{\\ell})\\) where \\(p\\) is the number of parameters, and \\(\\hat{\\ell}\\) is the log-likelihood evaluated at the estimated model parameters. The correction factor or penalty term \\(2p\\) penalizes complex models. AIC is often used in model comparison. Smaller the AIC, better is the fit to the data. Complex models are more penalized, of course.\nCross-Validation is a very popular hyper parameter tuning technique among ML community. It is worth noting that, asymptotically, AIC and LOOCV (Leave-one-out Cross Validation) are equivalent. See An Asymptotic Equivalence of Choice of Model by Cross Validation and Akaike’s Criterion[1977].\n\n# AIC\nprint('aic', model.aic)\n\naic 110.84897972060813\n\n\n\n\nLLR and Deviance\nThe simplest model one can fit to the data is to not have any features or just fit an intercept model. Then, we have two models \\(M_r \\equiv \\log \\left(\\frac{\\pi_i}{1-\\pi_i} \\right) = \\beta_0\\) where \\(\\beta_0\\) is the intercept term and no features are in the model. Here \\(M_r\\) stands for the reduced model.\nWhereas \\(M_f \\equiv \\log \\left(\\frac{\\pi_i}{1-\\pi_i} \\right) = x_i^T\\beta\\) defined earlier is the full model, meaning, all features are used in the model and perhaps, this is the best in the model class available like an Oracle. One may say, $M_r M_f $ when set of \\(x_i\\) includes the constant \\(1\\) as one of the features and both \\(M_r\\) and \\(M_f\\) belong to the same model class. The difference in the log-likelihoods is a useful indicator of how large the discrepancy between \\(M_r\\) and \\(M_f\\) is.\nThe Log-likelihood Ratio (LLR) is a statistic that computes this quantity:\n\\[LLR(M_r, M_f) = \\log\\left( \\frac{\\ell(M_r;D)}{\\ell(M_f;D)} \\right) = \\log(\\ell(M_r;D))-\\log(\\ell(M_f;D))\\]\n\n# LLR\nprint('LLR', model.llr)\n\nLLR 3.8544857609679326\n\n\nLLR expressed differently with a scaling factor is known as the Deviance defined as: \\[D(M_r, M_f) = -2 \\left[\\log(\\ell(M_r;D))-\\log(\\ell(M_f;D))\\right]\\]\nAsymptotically, Deviance follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom (which is the difference in the parameters between the full and the reduced models). So, we can see how useful the predictors are in \\(M_f\\) compared to \\(M_r\\). The evidence can be expressed in terms of p-value.\n\n# Wald's LLR test\n\nprint('LLR Test', model.llr_pvalue)\n\nLLR Test 0.04961313336509171\n\n\nWe reject the null (i.e. prefer \\(M_f\\), a more complex model over a simpler model \\(M_r\\)) at type-1 error rate \\(\\alpha\\) if \\(p_{val} &lt; \\alpha\\). Typical choice for \\(\\alpha\\) is 0.05.\n\n\n\\(\\nu\\text{-information criteria}\\)\nAt the core of this procedure is a way to, given the same data, compare two different models. Generally, one model will be simpler than the other. This idea is recently explored in the paper Understanding Dataset Difficulty with \\(\\nu\\)-Usable Information. The motivation for the authors was different, however. They want to characterize the dataset difficulty.\nThe paper introduces two new information-theoretic measures called \\(\\nu\\text{-information}\\) denoted as \\(I_{\\mathcal{V}}(X \\rightarrow Y)\\) and \\(\\text{pointwise }\\nu\\text{-information}\\) denoted as \\(\\text{PVI}(x \\rightarrow y)\\). Note that \\(I_{\\mathcal{V}}(X \\rightarrow Y) = \\mathcal{E}_{x,y \\sim P(X,Y)}[PVI(x,y)]\\).\nPlease see the paper for definitions and the treatment. But informally, from information theory point of view, \\(\\nu\\text{-information}\\) is the information gain due to conditioning (observing X). As a matter of fact, it closely resembles mutual information \\(I(X,Y) = H(Y)-H(Y|X)\\) where \\(H(X)\\) is the entropy of \\(Y\\) and \\(H(Y|X)\\) is the conditional (on \\(X\\)) entropy of \\(Y\\). But the difference is in how \\(X\\) and \\(Y\\) are related. \\(\\nu\\text{-information}\\) restricts the mappings \\(f: X \\rightarrow Y\\) to the admissable class of functions that can be learnt under the hypothesis class \\(\\mathcal{V}\\) and not any function which is used in classical mutual information (in fact, it is not specified).\nThe procedure to estimate \\(\\nu\\text{-information}\\) is given in Algorithm 1. After adapting the notations, \\(PVI\\) and \\(\\nu\\text{-information}\\) can be estimated from data as:\n\\[\n\\hat{PVI}(x_i,y_i) =  -\\log \\hat{\\ell}(M_r; x_i,y_i) + \\log \\hat{\\ell}(M_f; x_i,y_i)\\\\\n\\hat{I}_{M_r,M_f}(X \\rightarrow Y) = \\sum_{i=1}^{n} \\hat{PVI}(x_i,y_i)\n\\].\nIf we scale it appropriately, we notice that, \\(D(M_r,M_f) = 2I_{M_r,M_f}(X \\rightarrow Y)\\). So, the \\(I_{\\mathcal{V}}(X \\rightarrow Y)\\) is actually \\(LLR\\) with an information theoretic lense, applied to modern deep learning context, more specifically to LLMs.\n\\(PVI\\) was used to identify mislabelled examples. Across datasets, and training regimes, examples with high \\(PVI\\) are generally accurate and low \\(PVI\\) examples are not. This suggests that one can use \\(PVI\\) to define the confidence in the predictions.\nIt is to be noted that the connection between Likelihood and Information Theory is not new. Prof. Manny Parzen’s works have demonstrated this connection between in Goodness-of-Fit Tests and Entropy. See for example, Entropy Interpretation of Goodness of Fit Tests(1983) and Goodness of Fit tests and Entropy(1990). Indeed, LLR is a Goodness-of-Fit Test and statsmodel API gives p-vlaue for this test, as we have seen before.\n\n\nPerplexity\nPerplexity, as the name implies, is about the element of surprise. Again, it is just a fancy word for a monotonic transformation of cross-entropy, widely used in the NLP/LLM community. Having trained an LLM, one wants to see how perplexing the observed data to the LLM. More perplexing the data (higher the cross entropy), less likely is the model to have generated data or put in a more relatable fashion, higher is the additional number of bits needed to code the data.\nLike the recent \\(\\nu\\text{-information}\\) criteria, cross-entropy and other information-theoretic metrics such as the KL-divergence are also used to study LLM performance. For example, in The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning, token distributions are compared. Multiple metrics such as KL-divergence, Jaccard Similarity, among others, are implemented see code. Using these metrics, they showed that LLM Alignment via RLHF mostly did stylistic modifications to the output than it did to the factual content.\nOnce log-likelihood is available, perplexity can be calculates as \\[\n\\text{perplexity} = 2^{H(P,Q)}\n\\] where \\(P\\) is the true distribution and \\(Q\\) is the distribution under proposed model. Since true \\(P\\) is unknown, we often replace it with its empirical version. When a model is fit, the log-likelhood is an estimate of the negative cross-entropy. Therefore, \\[\n\\text{perplexity} = 2^{-\\log \\hat{\\ell}(M; D)}\n\\] where \\(M\\) is the model under consideration.\nIt is possible to compute cross-entropy between two models, where \\(P\\) takes the role of a reference distribution and \\(Q\\) takes the role of a probing model. Note that, this metric is no symmetric in \\(P\\) and \\(Q\\).",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sample Fitness</span>"
    ]
  },
  {
    "objectID": "notebooks/Conformal-Predictions.html",
    "href": "notebooks/Conformal-Predictions.html",
    "title": "Conformal Prediction",
    "section": "",
    "text": "We will use conformalization techniques to characterize the uncertainty so that certain statistical guarantees can be provided for the predictions. Most importantly, we will predict sets (not points). A set is general mathematical object that subsumes both intervals, labels, and categories.\nFurther, conformal procedures allow us to tune classifiers in a much more principled way. The traditional way of tuning a classifier is to plot the ROC curves and pick an operating point. The levers typically are precision, recall, fpr, tpr, etc.\nTuning a binary classifier is actually straightforward. But how does one pick a point or sets of points when there are more than two classes.\nLater we will see, the coverage probability and conformalization procedures turn this multivariate decision problem into a single variable decision problem.\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# we will use only two features but all the classes\niris = load_iris()\ny = iris.target\nX = iris.data[:, 0:2]\n\n# look at the data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_ratio = 0.70\ncalibration_ratio = 0.15\ntest_ratio = 0.15\n\n# train is now 70% of the entire data set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, shuffle=True)\n\n# test is now 10% of the initial data set\n# validation is now 15% of the initial data set\nX_cal, X_test, y_cal, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + calibration_ratio),shuffle=True) \n\nprint(X_train.shape, X_cal.shape, X_test.shape)\nprint(y_train.shape, y_cal.shape, y_test.shape)\n\n(104, 2) (23, 2) (23, 2)\n(104,) (23,) (23,)\n\n\n\n# Fit a logistic regression model\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0,multi_class='multinomial').fit(X_train, y_train)\nyh_train = clf.predict(X_train)\n\nfrom sklearn.metrics import classification_report\nprint('on train set')\nprint(classification_report(y_train, yh_train, target_names=iris.target_names))\n\nyh_test = clf.predict(X_test)\nprint('on test set')\nprint(classification_report(y_test, yh_test, target_names=iris.target_names))\n\non train set\n              precision    recall  f1-score   support\n\n      setosa       1.00      0.97      0.99        34\n  versicolor       0.70      0.81      0.75        37\n   virginica       0.75      0.64      0.69        33\n\n    accuracy                           0.81       104\n   macro avg       0.82      0.81      0.81       104\nweighted avg       0.81      0.81      0.81       104\n\non test set\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00         6\n  versicolor       0.75      0.43      0.55         7\n   virginica       0.69      0.90      0.78        10\n\n    accuracy                           0.78        23\n   macro avg       0.81      0.78      0.78        23\nweighted avg       0.79      0.78      0.77        23\n\n\n\n/opt/miniconda3/envs/ai839/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n# Implement Expected Calibration Error\n# Ref: https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d)\n\n\ndef expected_calibration_error(samples, true_labels, M=5):\n    # uniform binning approach with M number of bins\n    bin_boundaries = np.linspace(0, 1, M + 1)\n    bin_lowers = bin_boundaries[:-1]\n    bin_uppers = bin_boundaries[1:]\n\n    # get max probability per sample i\n    confidences = np.max(samples, axis=1)\n    # get predictions from confidences (positional in this case)\n    predicted_label = np.argmax(samples, axis=1)\n\n    # get a boolean list of correct/false predictions\n    accuracies = predicted_label==true_labels\n\n    ece = np.zeros(1)\n    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n        # determine if sample is in bin m (between bin lower & upper)\n        in_bin = np.logical_and(confidences &gt; bin_lower.item(), confidences &lt;= bin_upper.item())\n        # can calculate the empirical probability of a sample falling into bin m: (|Bm|/n)\n        prob_in_bin = in_bin.mean()\n\n        if prob_in_bin.item() &gt; 0:\n            # get the accuracy of bin m: acc(Bm)\n            accuracy_in_bin = accuracies[in_bin].mean()\n            # get the average confidence of bin m: conf(Bm)\n            avg_confidence_in_bin = confidences[in_bin].mean()\n            # calculate |acc(Bm) - conf(Bm)| * (|Bm|/n) for bin m and add to the total ECE\n            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prob_in_bin\n    return ece\n\n# ece on train set\npred_proba_train = clf.predict_proba(X_train)\nece_train = expected_calibration_error(pred_proba_train, y_train, M=10)\nprint('ece on train set',ece_train)\n\npred_proba_test = clf.predict_proba(X_test)\nece_test = expected_calibration_error(pred_proba_test, y_test, M=5)\nprint('ece on test set',ece_test)\n\nece on train set [0.07482647]\nece on test set [0.11760297]\n\n\n\n# Wrap the trained classifier into PUNCC\n\n# Create a wrapper of the random forest model to redefine its predict method\n# into logits predictions. Make sure to subclass BasePredictor.\n# Note that we needed to build a new wrapper (over BasePredictor) only because\n# the predict(.) method of RandomForestClassifier does not predict logits.\n# Otherwise, it is enough to use BasePredictor (e.g., neural network with softmax).\n\nfrom deel.puncc.classification import RAPS\nfrom deel.puncc.api.prediction import BasePredictor\nfrom deel.puncc.metrics import classification_mean_coverage\nfrom deel.puncc.metrics import classification_mean_size\n\nimport numpy as np\n\nclass LogisticPredictor(BasePredictor):\n    def predict(self, X, **kwargs):\n        return self.model.predict_proba(X, **kwargs)\n\n\n# Wrap model in the newly created RFPredictor\nclf_predictor = LogisticPredictor(clf)\n\n\n# CP method initialization\nraps_cp = RAPS(clf_predictor, train=True, k_reg=2, lambd=0)\n\n# The call to `fit` trains (with flag True) the model and computes the nonconformity\n# scores on the calibration set\nraps_cp.fit(X_fit=X_train, y_fit=y_train, X_calib=X_cal, y_calib=y_cal)\n\n\n# The predict method infers prediction intervals with respect to\n# the significance level alpha = 20%\ny_pred, set_pred = raps_cp.predict(X_test, alpha=.1)\n\n# Compute marginal coverage\ncoverage = classification_mean_coverage(y_test, set_pred)\nsize = classification_mean_size(set_pred)\n\nprint(f\"Marginal coverage: {np.round(coverage, 2)}\")\nprint(f\"Average prediction set size: {np.round(size, 2)}\")\n\nMarginal coverage: 0.87\nAverage prediction set size: 1.7\n\n\n/opt/miniconda3/envs/ai839/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n# Let us subset data s.t we will decide to predict if the set size is 1. o.w we will abstain\n# for illustration, we will do it on train set.\n\nfrom sklearn.metrics import accuracy_score\ny_pred, set_pred = raps_cp.predict(X_train, alpha=.1)\nind = np.array([len(x)==1 for x in set_pred])\n\nXsub_train = X_train[ind,:]\nysub_train = y_train[ind]\nyh_sub_train = clf.predict(Xsub_train)\nacc_sub = accuracy_score(ysub_train,yh_sub_train)\nacc = accuracy_score(y_train,clf.predict(X_train))\nprint('acc before', acc, 'acc after', acc_sub)\n\nacc before 0.8076923076923077 acc after 0.9459459459459459\n\n\n\n# we will vary alpha, compute accuracy on withheld sets\nalphas = np.linspace(0.05,0.4,num=10)\nacc_sub = []\nrej_ratio = []\n    \ndef get_acc(X,y, alpha):\n    acc_sub = []\n    rej_ratio = []\n    y_pred, set_pred = raps_cp.predict(X, alpha=alpha)\n    ind = np.array([len(x)==1 for x in set_pred])\n    X_sub = X[ind,:]\n    y_sub = y[ind]\n    yh_sub = clf.predict(X_sub)\n    acc = accuracy_score(y_sub,yh_sub)\n    rej = 1-sum(ind)/len(ind)\n    return acc, rej\n\n\nfor alpha in alphas:\n    acc, rej = get_acc(X_train,y_train, alpha)\n    acc_sub.append(acc)\n    rej_ratio.append(rej)\n\nplt.plot(alphas, acc_sub,label='Acc')\nplt.plot(alphas, rej_ratio, label='Abstention Rate')\nplt.legend()\nplt.xlabel('alpha')\n\nText(0.5, 0, 'alpha')\n\n\n\n\n\n\n\n\n\n\nplt.plot(rej_ratio, acc_sub)\nplt.xlabel('abstention rate')\nplt.ylabel('accuracy')\n\nText(0, 0.5, 'accuracy')",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conformal Prediction</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html",
    "href": "resources/project-card.html",
    "title": "Project Card",
    "section": "",
    "text": "Business View",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#background",
    "href": "resources/project-card.html#background",
    "title": "Project Card",
    "section": "Background",
    "text": "Background\nProvide succinct background to the problem so that the reader can empathize with the problem.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#problem",
    "href": "resources/project-card.html#problem",
    "title": "Project Card",
    "section": "Problem",
    "text": "Problem\nWhat is the problem being solved?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#customer",
    "href": "resources/project-card.html#customer",
    "title": "Project Card",
    "section": "Customer",
    "text": "Customer\nWho it is for? Is that a user or a beneficiary? What is the problem being solved? Who it is for?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#value-proposition",
    "href": "resources/project-card.html#value-proposition",
    "title": "Project Card",
    "section": "Value Proposition",
    "text": "Value Proposition\nWhy it needs to be solved?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#product",
    "href": "resources/project-card.html#product",
    "title": "Project Card",
    "section": "Product",
    "text": "Product\nHow does the solution look like? It is more of the experience, rather how it will be developed.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#objectives",
    "href": "resources/project-card.html#objectives",
    "title": "Project Card",
    "section": "Objectives",
    "text": "Objectives\nBreakdown the product into key (business) objectives that need to be delivered? SMART Goals is useful to frame\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#risks-challenges",
    "href": "resources/project-card.html#risks-challenges",
    "title": "Project Card",
    "section": "Risks & Challenges",
    "text": "Risks & Challenges\nWhat are the challenges one can face and ways to overcome?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#task",
    "href": "resources/project-card.html#task",
    "title": "Project Card",
    "section": "Task",
    "text": "Task\nWhat type of prediction problem is this? Link Model Card when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#metrics",
    "href": "resources/project-card.html#metrics",
    "title": "Project Card",
    "section": "Metrics",
    "text": "Metrics\nHow will the solution be evaluated - What are the ML metrics? What are the business metrics? Link Model Card when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#evaluation",
    "href": "resources/project-card.html#evaluation",
    "title": "Project Card",
    "section": "Evaluation",
    "text": "Evaluation\nHow will the solution be evaluated (process)? Link Model Card when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#data",
    "href": "resources/project-card.html#data",
    "title": "Project Card",
    "section": "Data",
    "text": "Data\nWhat type of data is needed? How will it be collected - for training and for continuous improvement? Link Data Cards when sufficient details become available (start small but early)\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#plan-roadmap",
    "href": "resources/project-card.html#plan-roadmap",
    "title": "Project Card",
    "section": "Plan/ Roadmap",
    "text": "Plan/ Roadmap\nProvide problem break-up, tentative timelines and deliverables? Use PACT format if SMART is not suitable.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#continuous-improvement",
    "href": "resources/project-card.html#continuous-improvement",
    "title": "Project Card",
    "section": "Continuous Improvement",
    "text": "Continuous Improvement\nHow will the system/model will improve? Provide a plan and means.\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "resources/project-card.html#resources",
    "href": "resources/project-card.html#resources",
    "title": "Project Card",
    "section": "Resources",
    "text": "Resources\nWhat resources are needed? Estimate the cost!\nyour response\n\nHuman Resources\nwhat type of team and strength needed?\nyour response\n\n\nCompute Resources\nWhat type of compute resources needed to train and serve?\nyour response",
    "crumbs": [
      "ML Documentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project Card</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Full Stack MLOps",
    "section": "",
    "text": "Content",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full Stack MLOps</span>"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Full Stack MLOps",
    "section": "Announcements",
    "text": "Announcements\n\n[01-January-2025] Material launched based on AI-839 taught at IIIT-B in the Fall of 2024.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Full Stack MLOps",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nExposure and skill in data handling, building models in Python, PyTorch\nExposure and skill in developing code using Python, Git, IDEs like VS Code\nA foundation course in Machine Learning, Deep Learning, Data Modeling, working with (Big) Data\n\nPart-1: Essentials (ML Engineering)\n\nTopics\n\nMLOps motivation, need\nBasic principles and MLOps with Open Source Software\n\nLearning Outcomes: students will be able to\n\nDeploy models with logging, documentation, unit tests, and APIs\nUnderstand a conceptual framework to approach MLOps holistically\n\n\nPart-2: Full Stack MLOps\n\nTopics\n\nHolistic understanding of ML development, beyond chasing typical performance metric\n\nLearning Outcomes: students will be able to\n\ndeploy models, observe their performance, make improvements, redeploy them.\nensure that the ML pipeline is reproducible.\nincorporate principles from Responsible AI and build ML systems which can consist of many models and tools.\nframe, discover, develop, deploy, monitor, improve, re-deploy and maintain an ML Application",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why MLOps?\nFirst and foremost, let us swallow the bitter pill - ML is just a piece of technology like any other to solve a business problem for which somebody needs to pay for. It means that unless people use it and pay for this technology (models), no matter how sophisticated and cool do they sound, they byte the dust like most ML initiatives do.\nNot just that - industry needs and loves determinism, accountability and reliability, which is the antithesis on which ML lives and thrives. MLOps is about addressing this inherent design challenge using a combination of tools and processes. Without which one has to appeal to luck to achieve the desirable. And we know from experience that luck favors the prepared.\nThis collection of notes on MLOps is about that preparation to manage the entire life cycle of an ML product, holistically and comprehensively. That is, for example, the ML system must be performant, explainable, reliable, fair and transparent, responsive and responsible, controllable, cost effective, collaborative, and many others, all which will discussed in detail later in the course.\n\n\nWhy A course on MLOps?\n\nUnderestimation of Technical Debt\n\nDeveloping ML systems involves significant technical complexities that are often underestimated, chiefly arising out of model-centric pedagogy.\nUnlike traditional software development, ML systems require continuous monitoring and iteration of the situated environment\n\nFundamental Differences from Traditional Software Development\n\nTraditional software development assumes a stable environment and known requirements.\nML systems, however, must adapt to changing environments and data, making it impossible to assume a perfect product at deployment.\n\nUncertain Data Environment\n\nAI systems inherently make mistakes and must be designed to handle continuous change and evolution.\nA good model in the lab does not guarantee performance in production, necessitating robust MLOps practices.\nThe deployment and monitoring of ML systems differ fundamentally from traditional software.\nContinuous monitoring and iteration are essential for maintaining model performance and relevance.\n\nOperationalizing ML\n\nOperationalizing ML is crucial for deriving value from AI systems, aligning them with business objectives and real-world applications. This can be a non-trivial activity, if not impossible in some cases.\nThis mindset is essential for understanding the practical aspects of ML deployment and maintenance.\n\n\nA course on MLOps exposes the developer or system designer to this varying challenges in the life cycle of ML (which is perpetual). This is entirely different from how one could solve an ML problem in academic settings where, more often than not, model novelty is incentivized over utility.\n\n\nWhy THIS course on MLOps?\nThere are many great resources available on the web in terms of books, blogs, courses, and tool documentation. Why yet another one? A legit question.\nAt a philosophical level - there are two objectives:\n\nFight the SOTA syndrome:\nLet me explain.\nThanks to chatGPT, it all seems to be about LLMs - building new models, spinning new shiny applications built around those LLMs both open and closed included. These new shiny objects exacerbated an already sick situation - a situation where building models (model.train) showing 0.01% (or even less) increase in accuracy is chased and cherished – all about State-of-the-Art (SOTA).\nWhile we must work hard and smart to move the needle, and it might be satisfying and fulfilling intellectually – unfortunately, it (chasing SOTA) is neither necessary nor sufficient to build reliable systems consisting of at least one ML component. There is lot more one has to consider in building and managing such systems.\nPrioritizes utility over novelty:\nAdmittedly, it is a boring job to be done. However, contrary to the prejudice, in the due course, you will learn that, indeed, solving for utility and overcoming the challenges along the way, is a rewarding journey in itself. Give no cult status to SOTA.\n\nMore concretely, there are some limitations of current MLOps Resources - they are notably:\n\nPractitioner-Focused Literature\n\nMost current MLOps books and resources are aimed at practitioners.\nThese resources focus heavily on tools and practices without structured assessment components.\nOften they anchor on specific framework/tool set.\n\nNeed for Academic Assessment\n\nIn academia, assessments are a crucial part of the learning process.\nThis material should not only teach concepts but also include tests to evaluate understanding.\nTesting is fundamental to learning and ensures that students grasp and can apply MLOps concepts effectively.\n\nComprehensive Curriculum\n\nThe course should cover not only MLOps but also ML system design and the operationalization of models.\nIntegrating these elements ensures a holistic understanding of both the development and deployment aspects of ML projects.\n\n\nSo the chief difference is - most of the resources are meant for practitioners. But this set of notes is meant both for students who can learn the tools and processes, and apply them and also for practitioners (who can learn the principles). But some of the challenges are more systemic and pervasive outlined below:\n\nVariability in MLOps Practices\n\nMLOps processes and practices vary significantly between individuals and organizations.\nUnlike DevOps in the software industry, which has become a mainstream discipline with standardized practices, MLOps is still relatively young.\n\nAbsence of Standard Vocabulary\n\nThere is no non-denominational vocabulary in MLOps, leading to inconsistencies in terminology and practices.\nStandardizing terminology and processes is crucial for effective communication and collaboration within and between organizations.\n\nKnowledge Diffusion and Skill Gap\n\nTraditional academic institutions typically propagate certain ideas and knowledge that get absorbed and amplified in the industry. But the reverse diffusion is often delayed.\nThere can be a lag between what academia offers and what the industry needs, especially in rapidly evolving fields like ML.\nset of templates (of code bases and also of materialized principles) that anybody can use in the Industry which improves the overall employee productivity of the employee\n\nNo Institutionalized MLOps Thinking\n\nInstitutionalizing MLOps in academic programs ensures that students are trained in industry-relevant skills from the start.\nThis approach can help standardize the quality and relevance of ML education, making graduates industry-ready.\nIntroduce a conceptual framework of models, actors and actions involved and a vocabulary to describe the complex ML dev cycle\nProvide a holistic view of ML - going beyond chasing performance metrics\npractice the principle theory-with-code and code-with-theory so that every principle is practicable, and every practice has a principle\ndevelop good (conceptual) models and principles which industry can adopt\n\n\n\n\nBenefits for Students\n\nIndustry Readiness\n\nInstead of learning these skills during internships or on the job, students will acquire them as part of their academic experience.\nThis prepares students to be industry-ready upon graduation, reducing the training burden on employers.\n\nHolistic Approach to Machine Learning\n\nThe course promotes a holistic view of ML, integrating model-centric and data-centric approaches.\nStudents will learn to consider all aspects of ML, including data quality, model robustness, and system integration.\n\nResponsible AI\n\nThe curriculum will cover aspects of responsible AI, ensuring that students are aware of ethical considerations and best practices in AI deployment.\nThis includes understanding biases, fairness, transparency, and accountability in ML models.\n\nComprehensive Skill Set\n\nStudents will gain a broad set of skills, from ML system design to operationalizing models.\nThis comprehensive skill set ensures that graduates are well-prepared to handle the full ML lifecycle in professional settings.\n\n\n\n\nBenefits for Academic Institutions\n\nPioneering Course Offering in India\n\nThis course is likely the first of its kind being offered in India, positioning the institution as a leader in ML education.\nBuilding on existing curricula such as Introduction to Machine Learning, Deep Learning, and Introduction to DevOps.\n\nIndustry-Relevant Education\n\nBy incorporating industry perspectives, including guest lectures and hands-on projects, the course aligns academic training with real-world needs.\nFold industry needs into the curriculum: students often gain experience in developing models and ML system design through internships, and this is transactional by design. This experiential knowledge has to be scaled with backward integration into the curriculum\nThis practitioner-oriented approach ensures students gain practical experience.\n\nEnhanced Employability\n\nAcademic institutions benefit by producing graduates who are immediately employable.\nStrong industry partnerships and placement opportunities can be developed, enhancing the institution’s reputation and appeal to prospective students.\nEquip students with essential skills: in ML lifecycle management, including deployment, monitoring, and automation\nPrepares students for roles such as ML engineer, MLOps engineer, and data scientist with operations expertise\nFamiliarize students with MLOps tools and platforms (e.g., Kubeflow, MLflow, TFX).\nEnhance the overall learning experience: By providing practical, hands-on experience with industry-relevant tools and practices\n\nCater to new market: As there is growing demand for MLOps skills in the industry due to the increasing adoption of AI and ML technologies. Organizations are looking for professionals who can manage the end-to-end ML lifecycle\n\n\n\nBenefits for Industry\n\nReducing Ad-Hoc Practices\n\nThe course aims to reduce the ad-hoc nature of MLOps practices and bring consistency to tooling choices.\nSimilar to how standardized practices in software development (e.g., Java build tools, POM) have streamlined processes, this course seeks to standardize MLOps practices.\n\nBuilding a Talent Pool\n\nThe industry will benefit from a pool of talent that is job-ready, reducing the onboarding time and training costs.\nGraduates will be proficient in MLOps practices, making them productive and profitable employees from the start.\n\nConceptual Frameworks for MLOps\n\nWhile the tools and techniques for implementing MLOps may vary, the course will provide students with robust frameworks and methodologies.\nThese frameworks will help students understand and apply MLOps principles in various contexts and adapt to new technologies as they emerge.\n\n\n\nThis course aims to introduce concepts that will stand the test of time, despite the rapid evolution of tools and techniques.\n\nBy focusing on foundational principles, the course provides a framework for thinking about MLOps that remains relevant as the field evolves.\n\n\n\nConsistency and Knowledge Transfer\n\nStandardized MLOps training ensures that professionals can move between projects with ease, facilitating better knowledge transfer and collaboration.\nThis reduces the time and effort needed to get new hires up to speed on MLOps practices within the organization.\n\n\n\n\nStyle\nContent will be presented in the form of take-away points, rather than main take-aways embedded in long winding paragraphs. Nuances etc will be added in the due course of time or video recordings will be made available.\n\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#discussions",
    "href": "course.html#discussions",
    "title": "Course",
    "section": "Discussions",
    "text": "Discussions\nWe will use WhatsApp group for (informal)discussions and alerts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#references",
    "href": "course.html#references",
    "title": "Course",
    "section": "References",
    "text": "References\n\n[book] ML Engineering, Andiry Burkov, 2019, LeanPub\n[book] Effective Data Science Infrastructure, Vile Tuulos, 2023, Manning\n[book] ML System Design, Chip Huyen, 2023, O’Reilly\n[course] CS329S @ Stanford: ML Systems Design, Chip Huyen, 2022\n[course] MLOps, Chip Huyen, 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#grading",
    "href": "course.html#grading",
    "title": "Course",
    "section": "Grading",
    "text": "Grading\n\n40%: Six assignments\n15%: Midterm mini project\n20%: In-class midterm\n25%: Capstone project",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#policies",
    "href": "course.html#policies",
    "title": "Course",
    "section": "Policies",
    "text": "Policies\n\nLate Submissions:  All deadlines are due at on the date and time indicated on the course page. The penalties for late submission are as follows:\n\nLate submissions not allowed (incur a zero) - except with prior approval or in valid exceptional cases\n\nMake-up Exam/Submission Policy: As per institute policy\nCitation Policy for Papers: Always mention the source, give full attribution and credits to citations, and as per institute policy\nAcademic Dishonesty/Plagiarism: As per institute policy\nAccommodation of Divyangs: As per institute policy\n\nSoma S Dhavala\nCourse Instructor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/w01-l01.html",
    "href": "lectures/w01-l01.html",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Materials:\nDate: Thursday, 01-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w01-l01.html#materials",
    "href": "lectures/w01-l01.html#materials",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\n\n\n\nIn-Class\n\nWe will go over W01-L01-Deck based on this from Chip Huyen’s CS329S: ML System Design Fall’22 @ Stanford\nSoftware 1.0 vs Software 2.0. See this Figure\n\n\n\nPost-class:\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper\nCh1-2 of AB, Ch1 of VT, Ch1 of CH1",
    "crumbs": [
      "Lectures[ML Engg.]",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w02-l01.html",
    "href": "lectures/w02-l01.html",
    "title": "02A: Grounding MLOps",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 06-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w02-l01.html#materials",
    "href": "lectures/w02-l01.html#materials",
    "title": "02A: Grounding MLOps",
    "section": "",
    "text": "Pre-work:\n\nRead the motivation for MLOps from this blog.\nReview W01-L01-Deck based on this from Chip Huyen’s CS329S: ML System Design Fall’22 @ Stanford\n\n\n\nIn-Class\n\nML Production Myths W01-L01-Deck\nSoftware 1.0 vs Software 2.0. See this W01-L02-Deck\nMLOps Motivation\nAI/ML Canvas from ml-ops.org Phase-Zero, Gokul Mohandas’s Project Canvas\n\n\n\nPost-class:\n\n[blog] A simple Tool to Start Making Decisions with the Help of AI\n[book, optional] Prediction Machines: The Simple Economics of AI, Ajay Agrawal, Joshua Gans, Avi Goldfare\n[blog] Why Metaflow\n[blog] Building and Managing Data Science Pipelines with Kedro from neptune.ai",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02A: Grounding MLOps"
    ]
  },
  {
    "objectID": "lectures/w02-l02.html",
    "href": "lectures/w02-l02.html",
    "title": "02B: Discover",
    "section": "",
    "text": "Materials:\nDate: Friday, 09-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02B: Discover"
    ]
  },
  {
    "objectID": "lectures/w02-l02.html#materials",
    "href": "lectures/w02-l02.html#materials",
    "title": "02B: Discover",
    "section": "",
    "text": "Pre-work:\n\nMLOps Motivation\nAI/ML Canvas from ml-ops.org Phase-Zero, Gokul Mohandas’s Project Canvas\nML Life Cycle CRISP-ML9(Q): blog, paper\n\n\n\nIn-Class\n\nDesign Thinking from Stanford Design School. It is a useful ideation tool to formulate a problem and develop a solution.\nProject Cards a Jupyter Notebook template for Project Cards. It touches the aspects of data-driven documentation and maintaining single source of truth for any document containing data. A WIP document on Data-driven ML Documentation. This also forms an important aspect of Transparency\nW02-L02 deck\n\n\n\nPost-class:\n\n[blog] MLOps Tech Stack\n[canvas] MLOps Canvas\n[site] Data Nutrition Project, CLeAR Framework paper, Data Nutrition Label paper\n[blog] 10 Dimensions of making data science work by Goda Ramkumar\n\nDimension 1: Expectation\nDimension 2: Strategy\nDimension 3: Roles\nDimension 4: Collaboration\nDimension 5: Culture\nDimension 6: Discovery and Access\nDimension 7: Platforms\nDimension 8: Scalability\nDimension 9: Methodology and Practices\nDimension 10: Quality and Governance [tbd]",
    "crumbs": [
      "Lectures[ML Engg.]",
      "02B: Discover"
    ]
  },
  {
    "objectID": "lectures/w03-l01.html",
    "href": "lectures/w03-l01.html",
    "title": "03A: Models for Modeling",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 13-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03A: Models for Modeling"
    ]
  },
  {
    "objectID": "lectures/w03-l01.html#materials",
    "href": "lectures/w03-l01.html#materials",
    "title": "03A: Models for Modeling",
    "section": "",
    "text": "Pre-work:\n\nMLOps Platforms some goals\nConvention over Configuration\nData Flow Paradigm\n\n\n\nIn-Class\n\nML model pipelines are Directed Acyclic Graphs. ML Workbench developed at EkStep was one of the early implementations of this primitive.\nW03-L01 deck\nKedro Tutorial: Part-0 and Part-1. Hands-on and commentary\n\n\n\nPost-class:\n\n[blog]Shreya Shankar on some MLOps Principles\n[git template] MLOps cookie cutter template for another opinionated project structure",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03A: Models for Modeling"
    ]
  },
  {
    "objectID": "lectures/w03-l02.html",
    "href": "lectures/w03-l02.html",
    "title": "03B: DevOps for ML",
    "section": "",
    "text": "Materials:\nDate: Friday, 16-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03B: DevOps for ML"
    ]
  },
  {
    "objectID": "lectures/w03-l02.html#materials",
    "href": "lectures/w03-l02.html#materials",
    "title": "03B: DevOps for ML",
    "section": "",
    "text": "Pre-work:\n\nMLOps Platforms some goals\nConvention over Configuration\nData Flow Paradigm\n\n\n\nIn-Class\n\nGit, Docker, Docker Compose, MinIO Object Store\nGit Actions, CI/CD\nKedro Tutorial Hands-on: Kedro pipeline execusion via airflow\nW03-L02\n\n\n\nPost-class:\n\n[Doc] Kedro-Docker Deploy Kedro pipline using Docker via Kedro-Docker plug-in\n[blog]Git Flow a mental model of how to work with Git, branching process, and feature development\n[video] Git actions for Metrics tracking\nFew additional resources on Git, Git Actions, Docker in the Tutorials chapter.\n[code] Data Science Life Cycle Process a git flow like Issue tracker and branching process more suitable for Data Science Projects.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "03B: DevOps for ML"
    ]
  },
  {
    "objectID": "lectures/w04-l01.html",
    "href": "lectures/w04-l01.html",
    "title": "04A: Develop",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 20-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04A: Develop"
    ]
  },
  {
    "objectID": "lectures/w04-l01.html#materials",
    "href": "lectures/w04-l01.html#materials",
    "title": "04A: Develop",
    "section": "",
    "text": "Pre-work:\n\nKedro Tutorial Part-0, Part-1.\n\n\n\nIn-Class\n\nA framework to think about metrics. For example, by focussing on who the the solution is for, metrics such as affordability, availability, accessibility, become definable.\nWorking with notebooks in Kedro (Kedro context magic)\nAPI documentation with quartodoc based on quarto, an alternative to sphinx\nTesting with PyTest.\nLinting and formating with ruff.\nW04-L01 Metrics slides #35-38 (more on metrics section)\n\n\n\nPost-class:\n\n[Deck] W04-L01. Review the material.\n[Docs] Kedro-Docker work with notebooks inside Kedro.\n[Docs] quartodoc Turn your docstrings into documentation with quartodoc. Plus any other content (notebooks, qmd files) can be rendered using quarto of course.\n[Docs] PyTest in Kedro\n[Docs] [ruff in Kedro](See Kedro docs",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04A: Develop"
    ]
  },
  {
    "objectID": "lectures/w04-l02.html",
    "href": "lectures/w04-l02.html",
    "title": "04B: Monitor (Data)",
    "section": "",
    "text": "Materials:\nDate: Friday, 23-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04B: Monitor (Data)"
    ]
  },
  {
    "objectID": "lectures/w04-l02.html#materials",
    "href": "lectures/w04-l02.html#materials",
    "title": "04B: Monitor (Data)",
    "section": "",
    "text": "Pre-work:\n\nSee how logistic regression test cases were written in sklearn. In particular, see how the test was prepared which makes it possible to test the fitted coeffcients analytically.\nSee how Peceptron test cases were written on Iris data.\n\n\n\nIn-Class\n\nQA for Data. It is all about asserting the statistical properties of the data.\nDiscussion on extracting statistical quality features into a table for any modality (tabular, image, speech, text)\nTesting columnar data with explicit conditions. Great Expectations defines them as expectations and validates them given a new data. For example, an expectation can be\n\na column can have at most 5% missing values\nthe range of the columns can be between [-2,10]\n\nTesting columnar data with implicit conditions. One dataset will be compared against a reference dataset. Evidently comes with many tests for reporting, model comparison, data drift detection. For example, we can compare whether or not the label distribution is same between the Train set and the Test set. A question for all readers - how often have you “actually” ran any statistical test to see if the Train and Test set are actually similar in distribution. My guess, less than 10% would have done it. The remaining would have called sklearn’s train_test_split function :)\n\n\n\nPost-class:\n\n[Blog] ETL testing with Great Expectations\n[Docs] Great Expectations Documentation. Please note that with version 1.0 released, even the examples from its repo are not working. Read them to understand what are typical tests on columnar data look like.\n[Notebooks] Evidently examples. Browse through and run how Evidently automates many test cases. Also see community examples here",
    "crumbs": [
      "Lectures[ML Engg.]",
      "04B: Monitor (Data)"
    ]
  },
  {
    "objectID": "lectures/w05-l01.html",
    "href": "lectures/w05-l01.html",
    "title": "05A: Monitor (Models)",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 27-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05A: Monitor (Models)"
    ]
  },
  {
    "objectID": "lectures/w05-l01.html#materials",
    "href": "lectures/w05-l01.html#materials",
    "title": "05A: Monitor (Models)",
    "section": "",
    "text": "Pre-work:\n\nReview Diagnostics of ML Systems from CS329s\n\n\n\nIn-Class\n\nRecap tests for monitoring data quality in the ETL stage\nUnderstand different types of drifts (label, covariate, concept) and ways to detect them that may be important in the model development stage\nCollecting data in the training stage - see Training Data, from CS329s\nData Programming - programmatically create labels. snorkel introduced these ideas first in the NLP space.\nSynthetic Data Generation using LLMs is a very promising and emerging application LLMs. For example, we can create labels of a piece of text, by prompting LLMs.\n\n\n\nPost-class:\n\nReview Training Data, from CS329s\nReview Feature Engineering, from CS329s",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05A: Monitor (Models)"
    ]
  },
  {
    "objectID": "lectures/w05-l02.html",
    "href": "lectures/w05-l02.html",
    "title": "05B: Deployment",
    "section": "",
    "text": "Materials:\nDate: Friday, 30-Aug-2024.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05B: Deployment"
    ]
  },
  {
    "objectID": "lectures/w05-l02.html#materials",
    "href": "lectures/w05-l02.html#materials",
    "title": "05B: Deployment",
    "section": "",
    "text": "Pre-work:\n\nmlflow docs\nkedro-mlflow tutorial\n\n\n\nIn-Class\n\nStream vs Batch Deployment\nHands-on Deploying models with mlflow on local server\n\n\n\nPost-class:\n\nReview Model Deployment from CS329s. [Note: Dr. Srinivas Rana will cover Model Compression, Quantization etc in his talk on Deployment on Edge]\nReview Monitoring and Continual Learning from CS329s\nReview Model Deployment chapter of ML Engineering book",
    "crumbs": [
      "Lectures[ML Engg.]",
      "05B: Deployment"
    ]
  },
  {
    "objectID": "lectures/w06-l01.html",
    "href": "lectures/w06-l01.html",
    "title": "06A: Evaluate",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 03-Sep-2024",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06A: Evaluate"
    ]
  },
  {
    "objectID": "lectures/w06-l01.html#materials",
    "href": "lectures/w06-l01.html#materials",
    "title": "06A: Evaluate",
    "section": "",
    "text": "Pre-work:\n\nReview Kolmogorov-Smirnov Test as a way to measure divergence between two continous, univariate distributions\nReview KL Divergence as a way to measure divergence between two arbitrary (continuous/discrete and univariate/multivariate) distributions\n\n\n\nIn-Class\n\nMotivation for why we need Design of Experiments and Hypothesis Testing. Where do they appear in the ML Life Cycle.\nLesson 1 - quick intro to DoE, scientific objectives, basic principles of DoEs, steps for planning, conducting, and analyzing an experiment.\nLesson 2 - a simple comparative experiment. The name A/B Testing, perhaps, comes from testing difference between two groups A and B, which is a simple comparative experiment. How to define a business problem as a hypothesis test, collect data, perform the test, draw conclusion are demonstrated. How to calculate the sample size, probably the most important question that gets asked, is also explained in this simple case.\n\n\n\nPost-Class\n\nReview Model Selection from CS329s\nReview Model Evaluation chapter of ML Engineering book\n\n\n\nReferences:\n\n[book] A/B Testing. This book gives a non technical introduction to A/B testing and how they get applied in the e-commerce, website UX optimization, running marketing campaigns. In the appendix, many scenarios of A/B testing are covered.\n[book] Design and Analysis of Experiments. This is a classic in DoE. Lessons 1-2 are necessary.\n[course]STAt 503 Design of Experiments - online course at UPenn\n[course]STAt 514 Design of Experiments - course at Purdue (stats oriented). Chapters 1-4 are needed.\n[book] Statistical Design. This is another classic from George Casella, a celebrated science author.\n\n\n\nNotes\n\nQA for Data discussed earlier is a specific case of A/B testing. CS folks call it A/B testing, but they are all different types of hypothesis tests.\nHypothesis Tests are tools for testing aspects of data. In the ML context, they can be used for testing\n\ndata drift (concept drift, covariate drift, label drift)\ndata quality (implicit and explicit)\nmodel performance\n\nOn Model Testing/ Comparison as a Hypothesis\n\nIs the “alternate” model better than the “baseline” model? Often, ML folks report performance metrics on Test split for all the models and pick the model with the highest performance. It is not uncommon to claim SOTA even when the performance difference is in 1/100ths of decimal places and replication is almost absent :). We do not even know if this difference is due to just chance (randomness in the data) alone. A rigorous (and perhaps, the right) approach would have been to formulate this as a hypothesis test, design an experiment, collect evidence and then conclude which model is better (and is statistically significant). Note that, statistical significance does not mean practical significance, which is often the case with most SOTA claims :). Another classical example to drive this point home is the (in)famous Netflix prize. The top performing model in the million dollar competition never made it to deployment. Find out why.\n\nHyperparameter Tuning and AutoML is a DoE in disguise\n\nthe experimental factors are, for example, the architecture, optimizer, learning rate, batch size, among others. The response variable is the performance. Techniques like Bayesian Optimization, and many techniques used in AutoML are indeed Sequential DoEs (you explore the search space sequentially by looking at the past exploration data). Grid-search, the naïve approach to hyperparameter tuning, can be seen as an implementation of a full-factorial DoE.\nWill ideas from DoE such as a Blocking lead to better search strategies? Can we find out if the learning rate and the optimizer (eg. Adam vs AdamW) interact with each other? The experiments to address these questions are often referred to as Ablation Studies in the ML community. So, by learning the principles of DoEs, we will be able to design (data and compute) efficient ablation studies.\n\nHow to plan and collect data for an ML problem?\n\nin the model-centric ML development, which most, if not all students, start their ML training in, will work on a data given to them in a platter. It is rare for an (undergraduate) student to have taken part in the data collection planning exercise. But once they are thrown into industry, they have to confront a very difficult question? We will address these questions in the next session.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06A: Evaluate"
    ]
  },
  {
    "objectID": "lectures/w06-l02.html",
    "href": "lectures/w06-l02.html",
    "title": "06B: Govern",
    "section": "",
    "text": "Materials:\nDate: Friday, 06-Sep-2024",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06B: Govern"
    ]
  },
  {
    "objectID": "lectures/w06-l02.html#materials",
    "href": "lectures/w06-l02.html#materials",
    "title": "06B: Govern",
    "section": "",
    "text": "Pre-work:\n\nCRUD- create, read, update and delete operations are the primitives we need to make the IT systems work.\nACID- against those CRUD operations, a database needs to support atomicity, consistency, isolation, durability to guarantee data validity, despite errors, and failures.\nSet Theory - is the mathematical basis for relational databases (RDBMS)\nGraph Theory - is probably the mathematical basis for graph databases like Neo4j. NoSQL databases might lie somewhere in the middle. Since Graphs are more general mathematical objects, we might think for now that, it is important to understand graph theory to design graph databases.\nD4M- Dynamic Distributed Dimensional Data Model - is a computer programming model that combines the advantages of distinct data processing technologies (sparse linear algebra, associate arrays, fuzzy algebra, distributed arrays, triple-store/NoSQL databases) to improve search, retrieval and analysis of data.\n\n\n\nIn-Class\n\nThe CRUD operations on Model Predictions\nThe R4 (Read, Replay, Recall, Replace) Framework for Level 5 Data Governance.\n\n\n\nPost-class\n\n[book] An Elementary Introduction to Statistical Learning Theory. See Chapter 8 for how the theory of VC Dimension can be applied to get an idea on the sample size.\n[site] GDPR - in particular focussing on privacy, and right to be forgotten (or right to erasure)\n[wiki] Self Driving Cars read the six levels (Level 0 to Level 5) of autonomy. Level 5 is fully autonomous self driving car.\n\n\n\nAdditional Reading (optional)\n\n[book] Mathematics of Big Data\n\n\n\nNotes\nAt at the heart of it, all of IT applications, eventually perform CRUD operations. Depending on the type of transaction, purpose, and SLAs, the choice of the database (SQL vs NoSQL), Batch-vs-Stream, OLAP vs OLTP, among others will be made. A whole lof of mathematical theories (set theory, graph theory, associative arrays, multi linear algebra, etc) are necessary to build the technologies.\nIn yet another simplified view, ML systems are glorified auto-fills. Take out the fundamental uncertainty in the data produced by ML systems, they are akin to IT systems. Therefore, same infrastructure and engineering process to support CRUD operations on ML produced data can be applied. But is that sufficient?\nOne major difference is - ML systems have memory. Therefore, deleting a record, does not lead to deleting its influence in downstream tasks right? A simple example will drive home the point. Imagine a model is trained on a dataset. There are some records which are leaking private data. What would deleting mean here. Obviously, one can delete those records from the training set. But one must also edit the models to get rid of their influence. Think one step further. What if, these marked records are very close the some other records. Deleting and retraining on the updated training data does not mean that their residual effect is removed. So, it is not only complex but also complicated. When ML models are cascaded and appear in a sequence of data events – controlling their exposure and affects requires a very good control on the downstream consumer applications.\nLet us try to re-interpret CRUD in the ML context. It leads us to the R4 {Read, Repeat, Recall, Replace} framework.\n\nRead\nOn demand, read (or retrieve) the prediction made by an ML model. This require maintaining proper metadata to retrive the records. For example, if the input (request to the API) is available, as is, and the output (response) of the API is logged into a persistent object store, this is possible.\n\n\nRepeat\nOn demand, repeat (or reproduce) the prediction made by an ML model. This require maintaining the three things for the sake reproducibility: 1. The data (inputs) and model artifacts 2. The code which loads the model, and scores on the given data and 3. The runtime to execute the code & data.\n\n\nRecall\nOn demand, recall the prediction made by an ML model. Depending on the “definition” of recall, the implementation and engineering complexity varies. In the simplest case which itself can be very complex, all past instances where the model scored, the predictions have to be replaced with say nulls. However, this implementation is not yet actionable by itself. The downstream consumer for example, can be notified of the recall, and take an appropriate action. The downstream consumer must have the Read ability in the R4 framework. In the best case scenario, the downstream consumer updates the decision (say with human-in-the-loop) or has another back-off strategy implemented already.\nIn the highest form of recall, all downstream consumers of the predictions are notified and decisions are propagaed in the entire chain of events.\n\n\nReplace\nOn demand, replace the prediction made by an ML model with another prediction (after correcting). Once the record is updated, in the most sophisticated case, all downstream consumers propagate the change downstream.\nLike in self-driving cars, the Level of Automation can be categorized.\n\n\n** R4 Levels**\n\n\n\n\n\n\n\n\nLevel\nName\nSupported Actions\nPlatform Features\n\n\n\n\nL0\nR0\nFire and Forget  Zero Traceability\nModel APIs\n\n\nL1\nR1  Read\nTrace past decisions\nObservability\n\n\nL2\nR2  Read & Repeat\nTrace and Recreate past decisions\nObservability +  Versioning + CD\n\n\nL3\nR3  Read, Repeat, Recall\nTrace, Recreate, and Nullify past decisions\nObservability  Versioning + CD +  CImp\n\n\nL4\nR3-N  Read, Repeat, Recall with Notify\nTrace, Recreate, Nullify, and Notify past decisions\nObservability  Versioning + CD +  CImp +  Event-IO\n\n\nL5\nR4  Read, Repeat, Recall, Replace\nTrace, Recreate, and Rescore past decisions\nObservability  Versioning + CD +  CImp\n\n\nL6\nR4-N  Read, Repeat, Recall, Replace with Notify\nTrace, Recreate, Rescore, and Notify past decisions\nObservability  Versioning + CD +  CImp +  Event-IO\n\n\n\n\nAre these four primitive operations sufficient to support any model governance? 1. Can a model be debiased? 2. Can right to be erased to enabled? 3. Many more.\nAs one can see, from an engineering stand point, certain platform features and functionalities are needed.\n\nRead: An observation platform to log data, with CRUD apis on the logs - can enable reading any past decisions (model predictions)\nRepeat: Versioning of data, models, code, and run time with Continuous Deployment (CD) can ensure reproducibility.\nRecall: A Continuous Improvement (CImp) capability that can take any alternate model (which can produce Nulls), deploy it, and rescore on any past data, can enable recalling past decisions.\nReplace: Same capability as Recall except that, a new model would correct past decisions.\nNotify: One has to make the memory-less system into a system with memory, in in async fashion. Implementing a pub-sub model, where downstream consumer subcribes to the R4 topics published by the prediction maker service. For example, a model service published a recall topic with all the decisions that need to be annulled in someway. The subscribers of this topic will take action however they deem fit. They can even publish another recall event and all of its subscribers can act accordingly. This way, a recall pipeline is created based on event triggers.",
    "crumbs": [
      "Lectures[ML Engg.]",
      "06B: Govern"
    ]
  },
  {
    "objectID": "lectures/w07-l01.html",
    "href": "lectures/w07-l01.html",
    "title": "07A: Scaling Laws",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 10-Sep-2024",
    "crumbs": [
      "Lectures[ML Science]",
      "07A: Scaling Laws"
    ]
  },
  {
    "objectID": "lectures/w07-l01.html#materials",
    "href": "lectures/w07-l01.html#materials",
    "title": "07A: Scaling Laws",
    "section": "",
    "text": "Pre-work:\n\nLesson 2 of Stat503 on planning a simple comparative experiment\n\n\n\nIn-Class\n\nReview sample size calculation in t-test. How sample size is a function of data quality, confidence, and tolerance for errors in the conclusions. Except in simple cases, getting a good sense of the “how much data” is a hard, rather very hard question. But we can try.\nMyth Buster - more data is better\n\nit is like a tautology. this notion never gets challenged. More data does not lead to better RoI. In fact, more of the same can never improve performance beyond a point or in some specific cases, it is impossible.\nlaw of diminishing returns. collecting more data can not only be expensive but can saturate, reaching a plateau. In fact, collecting data to understand where this plateau and what is the ceiling is, is an interesting problem in itself. A well designed experiment can address this question.\n\nCan we estimate the sample size needed?\n\nback of the envelope calculations based on some idea about the data, based on two-sample sample size calculations\nfrom PAC theory bounds (Chapter 8 of An Elementary Introduction to Statistical Learning Theory)\nempirical scaling laws\n\n\n\n\nPost-class\n\n[book] An Elementary Introduction to Statistical Learning Theory. See Chapter 8 for how the theory of VC Dimension can be applied to get an idea on the sample size.\n[youtube] PAC Learning Ali Ghosi Lec 19 PAC Learning, STAT 441/841 Statistical Learning, University of Waterloo\n[paper] Training Compute-Optimal LLMs\n\n\n\nAdditional Reading (optional)\n\n[book-online] Linear Models - first regression course for EPFL mathematicians.\n[paper] Approximation Rates and VC-Dimension Bounds for (P)ReLU MLP Mixture of Experts\n[book] Understanding Machine Learning: From Theory to Algorithms - a classic from Shai Shalev-Shwartz and Shai Ben-David. Part-1. youtube playlist\n[paper] Scaling Laws for Neural Language Models\n[paper] An empirical study of scaling laws for transfer learning\n[paper] Scaling laws for Individual data points\n[paper] Scaling laws in Linear Regression",
    "crumbs": [
      "Lectures[ML Science]",
      "07A: Scaling Laws"
    ]
  },
  {
    "objectID": "lectures/w07-l01.html#notes",
    "href": "lectures/w07-l01.html#notes",
    "title": "07A: Scaling Laws",
    "section": "Notes",
    "text": "Notes\nWhen embarking on a problem, the first set of questions we are asked are:\n\nWhat type/kind of data is needed or useful for the problem at hand?\nHow many samples do you need? A new name for this is scaling laws :)\nHow hard the problem is - are all examples will be equally important?\n\nLet us briefly discuss the first question about “what kind”.\n\nWhat to collect?\nWe discussed ideation tools like Design Thinking, Human-centered Design to engage with the problem (to define what to solve). In the Project Canvas, we also discussed what is the ML Task, and what kind of data is needed to solve the business problem.\nFor example, suppose the problem is, say, assessing the quality of food grains. The business problem is to objectively assess the grain quality and decide a fair price linked to the quality. ML problem is to grade the grains, identify any foreign materia/contaminations, may be based on the morphology of the grains. What kind of data is needed here? We can treat this problem as a combination of segmentation, object detection, object classification, morphology analysis, and granulometry. Data must support these tasks. See the blogs Image Annotation Definition, Usecases and Types, Best practices for successful image annotation, Data Annotation - A beginner’s guide to delve into annotating images further.\nArguably, annotating images for segmentation at grain level is very time consuming but can potentially be very useful. Treating this problem as a grading problem (accept/ reject) as opposed to provide quality assessment at grain and sample level is relatively easy in terms of annotations. But with accept/reject the lot binary labels at image level one can not pivot to a different ML problem formulation if the solution is proved not useful. This is an inherent design challenge. Getting as many details as possible is desirable but many not be practical. But thankfully, with foundation models like SAM, some of this leg work can be automated. See this paper on sample selection for efficient image annotation. Most modern annotation studios like Labellerr offer some support to speed-up the process.\nWhile the above labels support the primary ML task, say the segmentation task, for example, they may not be sufficient to offer SLAs or Model monitoring (battling the unknown unknowns) that we discussed in Model Monitoring. Suppose, for the same problem, the model is deployed in two grain collection sites. Training data was collected from the the first site (for rice grains), and model is being is used in the second site. It so happens that the second site is using the model for “wheat” instead of “rice.” Obviously, model will perform poorly. This may not have been anticipated ahead of time. Only way to detect this is to first log the data where the model is being used, and observe the performance by site. That means, where the model is being in this case and any other contextual data is necessary to monitor, diagnose, and improve models over time. This is all but one of the dimensions to think about. Generally speaking, thinking about dimensions is the topic of knowledge representation and knowledge engineering. Dimensional Modeling](https://www.ibm.com/docs/en/ida/9.1.2?topic=modeling-dimensional) is a framework to adopt to collect data that can address the concerns of different stakeholders besides the ML and Data Scientists such as a Software Engineer, QA, Architect, Product Manager, Site Manager etc. IBM’s book on Dimensional Modeling: In a Business Intelligence Environment is a definitive guide on this topic. A knowledge Engineering Primer is perhaps a lighter compared to the book.\nNow let us move to the other question. How many?\n\n\nHow many?\nThis question gets very complicated in no time. And there can be more than one way to develop a heuristic to arrive at an approximate answer. The type of response and approach also depends on the scenario.\n\nExact problem was solved before or data is available which can be used as-is [best case]. Take the data, build a model, and deploy and start using. Such occurrences can be rare but can happen. For example, people counting of a given demographic from traffic camera feeds.\nSimilar problem solved, except for differences in domain. Take a model trained on that data, collect data on the target domain. Start improving the model over a period of time. Use transfer learning/ supervised fine-tuning methods. If data exists (in literature or on huggingace/kaggle for eg) for that problem, plot accuracy vs sample size and pick a target that gives desired performance. Iterate over it. Do not collect all data at once.\nUnlabelled data available in large quantities. In such a case, run a pre-trained model or develop self-supervised learning tasks, get very good representations and try to label those that are easy and/or important for the performance. See cords and other active learning methods. See the application of self-supervised pre-training techniques for multi-lable classification in Chest X-Ray paper\nData exists and similar problem is not solved before [worst case]. Then, one has to design an experiment, provide some inputs (design considerations or operating environment) about the problem, get a ball park sense of the sample size (for the sake of budget, resource and project planning), run a pilot data collection exercise, refine the strategy and iterate. Below, we develop some heuristics to estimate the sample size.\n\n\nA Statistical Approach\nLet us simplify and consider a regression problem. We are trying to learn a function \\(f: [0,1] \\rightarrow R\\). Imagine you are fitting a decision tree to approximate this function. A decision tree partitions the input space, and in each of the partitions certain statistics like mean and quintiles are computed. For a given instance, the prediction is given by, for example, the mean of all responses of the examples belonging to that partition. So, we can divide or cluster or partition the training data into \\(K\\) subsets and compute some statistic in each these subsets. If unlablled data is available, running a clustering algorithms will give an idea about \\(K\\). If we assume that the labels (responses) of the k-th partition denoted by \\(y_k\\) follow \\(N(\\mu_k, \\sigma^2)\\), we can estimate the \\((1-\\alpha)\\) level prediction interval (PI) for \\(\\mu_k\\) as \\[\\bar{y}_k + z_{1-\\frac{\\alpha}{2}}\\sigma \\sqrt{1+\\frac{1}{N}}\\]\nwhere \\(\\bar{y}_k\\) is the sample mean, \\(N\\) is the sample size, \\(\\sigma^2\\) is the noise variance, \\(\\alpha\\) controls the confidence level (or type-1 error of the corresponding hypothesis test). So the “design inputs” needed to solicit a sample size are: \\(\\alpha\\), \\(\\sigma^2\\). Sometimes, the precision needed for the estimate can be asserted in terms of the width of the interval (PIW). In this case, PIW is given as \\(PIW = 2z_{1-\\frac{\\alpha}{2}}\\sigma \\sqrt{1+\\frac{1}{N}}\\). Now, we can express sample size as a function of \\(PIW\\), \\(\\alpha\\) as: \\(N = \\left(  (\\frac{PIW}{2z_{1-\\frac{\\alpha}{2}}})^{2}-1 \\right)^{-1}\\). If there are \\(K\\) partitions, we need to estimate that many \\(\\mu_k\\)s. So, the total sample size will be \\(NK\\) assuming all partitions have same variance. If not, is is not hard to update the formula. In someways, the model complexity is captured by \\(K\\). In general, one does not know these numbers in advance and has to make an educated guess based on domain knowledge and refine the design inputs as the data collection drive is set in motion.\nWhat about the classification problem?\nAssume it is a binary classification problem. Approach is still identical. Even in the classification setting, estimating the mean and taking argmax to predict the label of the partition is still useful and applicable except that the \\(PI\\) formula needs to be updated. For other types of Tasks, suitable estimate of the target varaible has to be chosen, and derive its PI, and use it to get an estimate of the sample size.\n\n\nAn ML Approach\nCan we relax the assumptions and yet come-up with some estimates for \\(N\\)?\nWe can invoke PAC theory. Suppose \\(\\epsilon\\) is the tolerable error (that the test error should not exceed), \\(\\delta\\) is the confidence in the learning algorithm that test error can exceed \\(\\epsilon\\) by no more than \\(\\delta\\) fraction of times and \\(V\\) is the VC-dimensionof the function class. The following bounds from PAC theory can guide us:\n\\[\\frac{1}{\\epsilon}\\left(4\\log(\\frac{2}{\\delta}) + 8V\\log(\\frac{13}{\\epsilon}) \\right) \\le N \\le \\frac{64}{\\epsilon^2}\\left(2V \\log(\\frac{V}{\\epsilon}) + \\log(\\frac{4}{\\delta}) \\right)  \\]\nIn particular, if one uses an MLP, following is a lower bound on the VC-dimension \\[V \\le 2(d+1)s \\log(2.718s)\\] where \\(d\\) is the number of features, \\(s\\) is the number of perceptrons in the network. So, in effect, given an MLP architecture (with \\(s\\) number of perceptrons), confidence(\\(\\delta\\)), permissible error (\\(\\epsilon\\)), input features (\\(d\\)), one can get an idea of the sample size.\nCaution: These bounds can be very vacuous and many not be uniformly tight across the range of the inputs. For example, consider the inequality \\(x^2 \\ge x, \\forall x \\ge 1\\). The gap \\(|x^2-x|\\) grows unboundedly and gets worse as \\(x\\) increases. So, always play with several input values and pick a sensible one. Do not just believe that they will work out of the box. No magics here.\n\n\nA DL Approach\nFor tabular problems, one can use either of the above methods to get a sensible estimate. But for speech, vision, and language datasets, it is both complicated and simple at the same time. Simple in the sense that, one could take pre-trained foundation models and work with their representations and technically use PAC-theory-based heuristics. But the latent dimensions could be extremely large. So, if unlabeled data is available, run a clustering algorithm and figure out the intrinsic dimensionality which can be plugged into previous formulae. It is complicated when one has no prior knowledge and commits to a Deep Learning approach. In such cases, building a simple baselines is still going to be useful. Such a simple model can guide the data collection exercise.\n\n\nWhat about Large Language Models?\nDo we want to train custom LLMs or do we want to use them for inference?\nFor training LLMs, how much data is needed, the compute needed and the performance and emergence of the variables to study play. This area is emerging with results and counter-results. But some early works fit parametric curves to predict performance given training data (counted in terms of number of tokens) and for a given compute budget. The following regression model is considered in Training Compute-Optimal LLMs, using their notation, known as Chinchila scaling laws : \\[ L(N,D) = \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}} + E \\] where \\(L()\\) is the loss, \\(D\\) is the training dataset size in tokens, \\(N\\) is the model size, \\(A,B,E,\\alpha, \\beta\\) are unknowns to be fit from experimental data. Under this mode, \\(E\\) is the smallest loss achievable (irreducible noise), with infinite data and infinite compute. Based on large scale experiments, they fit \\[ L(N,D) = \\frac{406.4}{N^{0.34}} + \\frac{410.7}{D^{0.28}} + 1.69 \\]. It may be better to use a more standard notation and rewrite them as: \\[ E(K,N) = \\frac{A}{K^{\\alpha}} + \\frac{B}{N^{\\beta}} + \\sigma^2 \\] where \\(K\\) is the model size/complexity, \\(N\\) is the number of tokens, and \\(\\sigma^2\\) is the noise variance. See Pathways Language Model and Model Scaling from Aakanskha for more on Scaling Laws.\nSome observations w.r.t LLM scaling laws are:\n\nTraining LLMs from scratch is an extremely specialized endeavor, requiring not only deep pockets, good understanding of the LLM science but also solid (distributed) systems engineering knowledge.\nBoth data size and model size of sufficient size are needed to see emergence.\nFor instruction fine-tuning, about 1k-6k instruction pairs is considered a good start. See LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigm . More would be better in this case. Like always, quality and representativeness matter.\nFor In-Context Learning (ICL), a fewshot learning is better. But instead of prompt engineering in ad-hoc fashion, optimizing prompts in a data driven manner, using frameworks like DSPy. For example, how many and which examples to include in the fewshot ICL can be optimzed with DSPy. Fewshot is all but one of the strategies to improve the performance of LLMs. See The Prompt report for more details.\n\n\n\nTake-aways\n\nCollecting data is an iterative exercise\nPlay with several design inputs and pick a good starting point. Run several heuristics.\nTry to leverage past knowledge (datasets, models, and problem similarity) as much as possible.\nDo not collect all data in one tranche but collect often, refine the strategy and iterate.\nIncorporate practical constraints. Otherwise, data collection will not even begin.",
    "crumbs": [
      "Lectures[ML Science]",
      "07A: Scaling Laws"
    ]
  },
  {
    "objectID": "lectures/w07-l02.html",
    "href": "lectures/w07-l02.html",
    "title": "07B: Sample Hardness",
    "section": "",
    "text": "Materials:\nDate: Friday, 13-Sep-2024",
    "crumbs": [
      "Lectures[ML Science]",
      "07B: Sample Hardness"
    ]
  },
  {
    "objectID": "lectures/w07-l02.html#materials",
    "href": "lectures/w07-l02.html#materials",
    "title": "07B: Sample Hardness",
    "section": "",
    "text": "Pre-work:\n\nAIC a criteria for model selection\ncords for a collection of works/implementations based on subset selection\n\n\n\nIn-Class\n\nCharacterizing data difficulty or sample hardness.\nLook at some statistics like Relative Mahalanobis Distance (which some used to flag OOD, others used to measure sample hardness), Perplexity (cross entropy between two models, one model and data or between), Trust Scores\nSample easiness on training performance and generalization error\nSee this notebook where we walk through these concept on a toy dataset\n\n\n\nPost-class\n\n[paper] Learning Sample Difficulty from Pre-trained Models for Reliable Prediction\n[paper] A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection\n[paper] Dissecting Sample Hardness: A fine-grained analysis of hardness characterization methods for data-centric AI\n[paper] To Trust or Not To Trust A Classifier\n\n\n\nAdditional Reading (optional)\n\n[paper] Understanding Dataset difficulty\n[tools] Pytorch-ood - a collection of techniques to detect OOD in PyTroch. Mostly image focussed.\n[tools] PyOD - a collection of anomaly detection techniques\n[tools] DEEL - a collection of OOD, XAI, and other techniques",
    "crumbs": [
      "Lectures[ML Science]",
      "07B: Sample Hardness"
    ]
  },
  {
    "objectID": "lectures/w07-l02.html#notes",
    "href": "lectures/w07-l02.html#notes",
    "title": "07B: Sample Hardness",
    "section": "Notes",
    "text": "Notes\n\nNot all examples are equal (in the eyes of the model). There can be many reasons.\nThere can be outliers (in the feature space, in the label space or both in the feature and label space).\nOutliers affect the model performance in different ways.\nA suite of techniques, preferably model-agnostic, are needed to quantify sample hardness and make them available at dataset level (train set) and also at inference time.",
    "crumbs": [
      "Lectures[ML Science]",
      "07B: Sample Hardness"
    ]
  },
  {
    "objectID": "lectures/w08-l01.html",
    "href": "lectures/w08-l01.html",
    "title": "08A: Model Fitness",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 17-Sep-2024",
    "crumbs": [
      "Lectures[ML Science]",
      "08A: Model Fitness"
    ]
  },
  {
    "objectID": "lectures/w08-l01.html#materials",
    "href": "lectures/w08-l01.html#materials",
    "title": "08A: Model Fitness",
    "section": "",
    "text": "Pre-work:\n\n[tools] Pytorch-ood - a collection of techniques to detect OOD in PyTroch. Mostly image focussed.\n[tools] PyOD - a collection of anomaly detection techniques\n\n\n\nIn-Class\n\nCharacterizing data difficulty or sample hardness : Understanding Dataset difficulty\n[notebook] Sample Fitness Metrics based on Information Theory where we walk through these concept on a toy dataset\nModel diagnostics of LLMs: Application of Random Matrix Theory (RMT) to assess model generalization ability\n\nImplicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning paper [JMLR, 2021]\nPredicting trends in the quality of state-of-the-art neural networks without accesss to training or testing data paper [Nature Communications, 2021]\nApplication of RMT to analyze an LSA (Latent Semantic Analysis) model notebook\n\n\n\n\nPost-class\n\n[paper] The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning\n[tools] AlignTDS - common metrics to detect differences between token distributions in LLMs\n[video] Heavy Tails in ML: Structures, Stability Dynamics Invited talk at NeurIPS’23\n[notebooks] RMT Application for Diagnosing LLMs. See the git repo for navigation.\n[blog] blog Explain double descent using Weight Watchers.",
    "crumbs": [
      "Lectures[ML Science]",
      "08A: Model Fitness"
    ]
  },
  {
    "objectID": "lectures/w08-l01.html#notes",
    "href": "lectures/w08-l01.html#notes",
    "title": "08A: Model Fitness",
    "section": "Notes",
    "text": "Notes\n\nMany metrics proposed to understand and characterize data and models are based on information theory.\nLikelihood Ratio, Deviance, Cross-Entropy, Perplexity, \\(\\nu\\text{-information}\\) all are related in linear and Generalized Linear models. They can be useful in modern deep learning and LLM context as well.\nWeight|Watchers is a very interesting application of Random Matrix Theory (RMT) to study the training dynamics of LLMs and other blackbox models. We need access to the model weights, not necessarily the entire training data. Fit power-law to the eigen spectrum of the weights, and based on the model (of learning dynamics), characterize the training regime into different stages. Models that have good generalization capabilities exhibit different characteristics in in the eigen spectrum.",
    "crumbs": [
      "Lectures[ML Science]",
      "08A: Model Fitness"
    ]
  },
  {
    "objectID": "lectures/w09-l01.html",
    "href": "lectures/w09-l01.html",
    "title": "09A: Uncertainty Quantification",
    "section": "",
    "text": "Materials:\nDate: Tuesday, 24-Sep-2024",
    "crumbs": [
      "Lectures[ML Science]",
      "09A: Uncertainty Quantification"
    ]
  },
  {
    "objectID": "lectures/w09-l01.html#materials",
    "href": "lectures/w09-l01.html#materials",
    "title": "09A: Uncertainty Quantification",
    "section": "",
    "text": "Pre-work:\n\n[blog] Expected Calibration Error\n[paper] Calibration in Deep Learning: A Survey of the State-of-the-Art\n[paper] On Calibration of Modern Neural Networks\n[tutorial] Introduction to Uncertainty in Deep Learning\n\n\n\nIn-Class\n\nA gentle introduction to Conformal Prediction and Distribution-free Uncertainty Quantification Video\ncolab from DEEL-PUNCC\n\n\n\nPost-class\n\n[paper] A tutorial on Conformal Prediction\n[paper] Towards Reliability using Pretrained Large Model Extensions\n[tools] awesome-conformal-prediction - a collection Conformal Prediction resources including implementations.\n[tools] crepes - Conformal Classifiers, Regressors, and Predictive Systems.\n[tools] TorchCP - a python toolbox for Conformal Prediction research in Deep Learning Models using PyTorch.\n[tools] MAPIE - a python toolbox for Conformal Prediction\n[tools] DEEL-PUNCC - a python toolbox for Conformal Prediction from DEEL.ai a project for Dependable, Certifiable, Explainable AI for Critical Systems. Checkout the sister projects from DEEl on Bias DEEL INFLUENCIAE, oodeel for OOD, xplique for XAI,",
    "crumbs": [
      "Lectures[ML Science]",
      "09A: Uncertainty Quantification"
    ]
  },
  {
    "objectID": "lectures/w09-l01.html#notes",
    "href": "lectures/w09-l01.html#notes",
    "title": "09A: Uncertainty Quantification",
    "section": "Notes",
    "text": "Notes\n\nDeep Learning models are not calibrated. They can make confident, but wrong mistakes.\nConformal Prediction (CP) provides a rigorous statistical guarantees for the predictions by predicting sets and not points. For example, in a regression problem, one gets to predict an interval with guaranteed coverage probability. In a classification problem, CP may predict more than one class label.\nCP is model-agnostic and can work for a variety of tasks including, regression, multi-class classification, multi-label prediction, time-series models, and also useful in LLMs Conformal Language Modeling, even though it is still a research topic.\nIt is a post-hoc technique and should be used in every project.",
    "crumbs": [
      "Lectures[ML Science]",
      "09A: Uncertainty Quantification"
    ]
  },
  {
    "objectID": "lectures/w11-l02.html",
    "href": "lectures/w11-l02.html",
    "title": "11B: Influence Functions",
    "section": "",
    "text": "Materials:\nDate: Friday, 25-Oct-2024",
    "crumbs": [
      "Lectures[ML Science]",
      "11B: Influence Functions"
    ]
  },
  {
    "objectID": "lectures/w11-l02.html#materials",
    "href": "lectures/w11-l02.html#materials",
    "title": "11B: Influence Functions",
    "section": "",
    "text": "Pre-work:\n\n[tools] DEEL-PUNCC - a python toolbox for Conformal Prediction from DEEL.ai a project for Dependable, Certifiable, Explainable AI for Critical Systems. Checkout the sister projects from DEEl on Bias DEEL INFLUENCIAE, oodeel for OOD, xplique for XAI,\nWhite Paper - Machine Learning in Certified Systems\n\n\n\nIn-Class\nTopic: Gradients &gt; IFs &gt; {Debugging, OOD, Robustness, XAI}\nDiscussion:\n\nCook’s Distance\n\nleverage\noutlier\ninfluence\n\nLeveraing Influence Functions for Dataset Exploration and Cleaning\nInterpreting Robust Optimization via Adversarial Influence Functions\nData Debugging: TraceIn - Estimating Training Data Influence by Tracing Gradient Descent\nGradOrth A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients\nAxiomatic Attribution for Deep Networks - integrated gradients for xai\nUnderstanding Black-box Predictions via Influence Functions - Perhaps, the first applications of Influence Functions to explain Black Box Deep Learning models\nInfluence functions in Machine Learning tasks\n\nLab\n\nIF from pyDVL library for data valuation and influence function computation.\nOOD: Notebook\nXAI in LLMs: Notebook Use IFs for XIA on LLMs, Blog\nXAI with gradients xplique\n\n\n\nPost-class\n\nInfluence Functions (IFs)\n\nTheory:\n\nHampel’s IF Seminal Paper - The Influence Curve and its Role in Robust Estimation, from Frank R. Hampel, that started this work\n\nComputation:\n\nScaling Up Influence Functions\nRevisiting inverse Hessian vector products for calculating influence functions\nM-FAC: Efficient Matrix-Free Approximations of Second-Order Information\nFastIF FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging, with some applications in NLP.\n\n(Early) Application in ML:\n\nUnderstanding Black-box Predictions via Influence Functions - Perhaps, the first applications of Influence Functions to explain Black Box Deep Learning models\nInfluence Functions in Deep Learning Are Fragile\nInfluence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations\nInfuence functions in Machine Learning tasks\nIf Influence Functions are the Answer, Then What is the Question?\n\nTools:\n\nInfluenciate from the DEEL Influenciae project\npyDVL library for data valuation and influence function computation.\n\n\n\n\nLLMs\n\nStudying Large Language Model Generalization with Influence Functions\nDo Influence Functions Work on Large Language Models?\nTextGrad Automatic ‘’Differentiation’’ via Text, paper\n\n\n\nXAI using Gradients and IFs\n\nAxiomatic Attribution for Deep Networks - integrated gradients for xai\nFor others DEEL.ai’ Xplique code paper\n\n\n\n\nData Debugging\n\nEstimating Training Data Influence by Tracing Gradient Descent\nLeveraing Influence Functions for Dataset Exploration and Cleaning\nUnderstanding Black-box Predictions via Influence Functions\nRelatIF: Identifying Explanatory Training Examples via Relative Influence\nOn the Accuracy of Influence Functions for Measuring Group Effects\nRepresenter Point Selection for Explaining Deep Neural Networks\nRepresenter Point Selection via Local Jacobian Expansion for Post-hoc Classifier Explanation of Deep Neural Networks and Ensemble Models\nUnderstanding Influence Functions and Datamodels via Harmonic Analysis\nAn Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data Make a Big Difference?\nLAVA: Data Valuation without Pre-Specified Learning Algorithms\n\n\nOOD\n\nOut-of-Distribution Generalization Analysis via Influence Function\nHow Useful are Gradients for OOD Detection Really? - a negative results on gradients\nGradient-Regularized Out-of-Distribution Detection\nGradOrth A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients\noodeel - a toolbox for OOD detection\n\n\n\nRobustness\n\nInterpreting Robust Optimization via Adversarial Influence Functions\nRobust inference: The approach based on influence functions\nRobust inference by influence functions\nGeneralized Influence Functions and Robustness Analysis",
    "crumbs": [
      "Lectures[ML Science]",
      "11B: Influence Functions"
    ]
  },
  {
    "objectID": "lectures/w11-l02.html#notes",
    "href": "lectures/w11-l02.html#notes",
    "title": "11B: Influence Functions",
    "section": "Notes",
    "text": "Notes\n\n\nGradients is all you need\n\n\nModality\nAspect\nTopics\nResources\n\n\n\n\nTabular\nOOD\nx\nx\n\n\nTabular\nXAI\nx\nx\n\n\nTabular\nBias\nx\nx\n\n\nTabular\nCP\nx\nx\n\n\nTabular\nRobustness\nx\nx\n\n\nText\nOOD\nx\nx\n\n\nText\nXAI\nx\nx\n\n\nText\nBias\nx\nx\n\n\nText\nCP\nx\nx\n\n\nText\nRobustness\nx\nx\n\n\nSpeech\nOOD\nx\nx\n\n\nSpeech\nXAI\nx\nx\n\n\nSpeech\nBias\nx\nx\n\n\nSpeech\nCP\nx\nx\n\n\nSpeech\nRobustness\nx\nx\n\n\nImage\nOOD\nx\nx\n\n\nImage\nXAI\nx\nx\n\n\nImage\nBias\nx\nx\n\n\nImage\nCP\nx\nx\n\n\nImage\nRobustness\nx\nx\n\n\nVideo\nOOD\nx\nx\n\n\nVideo\nXAI\nx\nx\n\n\nVideo\nBias\nx\nx\n\n\nVideo\nCP\nx\nx\n\n\nVideo\nRobustness\nx\nx",
    "crumbs": [
      "Lectures[ML Science]",
      "11B: Influence Functions"
    ]
  },
  {
    "objectID": "lectures/w13-l01.html",
    "href": "lectures/w13-l01.html",
    "title": "13A: LLMs Introduction",
    "section": "",
    "text": "Materials:\nDate: Thursday, 07-Nov-2024, 11.30-1pm, IST.",
    "crumbs": [
      "LLMs-Ops",
      "13A: LLMs Introduction"
    ]
  },
  {
    "objectID": "lectures/w13-l01.html#materials",
    "href": "lectures/w13-l01.html#materials",
    "title": "13A: LLMs Introduction",
    "section": "",
    "text": "Pre-work:\n\nLING571@UW Deep Learning For NLP, Prof. Shane at UW, Spring’24. Introduction, Word Vectors, Language Modeling\nCIS7000@UPenn LLMs, by Prof. Mayur Naik at UPenn, Fall’24, Background, Language Modeling\nAIL821@IIT-Delhi LLMs: Introduction and Recent Advances ELL881/AIL821, LLMs: Introduction and Advances @ IIT-Delhi, Fall’24.\nTransformers\n\nLLMs @ UPenn Part-1, Part-2\nLLMs: Introduction and Recent Advances @ IIT Delhi Module-5 on RNNs, Module-6 on Attention and Transformers\n\n\n\n\nIn-Class\nWe will follow the “Follow the data” approach to organize the content.\n\nQuick review of NLP and Deep Learning for NLP, pre- and post-GPT world.\n\nLecture 2 from AIL821 Introduction to NLP,\nLecture 3.1 from AIL821 Introduction to Language Models\n\nLLM Flow: (Quality) Datasets, Model Training (Pre-training, Alignment, Fine-tuning), Prompt Optimization, Constrained Language Generation, Evaluation.\nDatasets and Tasks (to train LLMs)\n\nLecture 7 from AIL821\n\nModel Training\n\nPre-training\n\nLecture 12.1 from AIL821\n\nAlignment\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model\n\nFine-tuning\n\nLoRA: Low-Rank Adaptation of Large Language Models\n\n\nPrompt Optimization\n\nChain-of-Thought\n\nConstrained Language Generation\n\nCollection\n\nEvaluation\n\nScaling Evaluation of LLMs Yann Bubois, CIS 7000 LLM Course\n\nApplications and Design Patterns\n\nTools\n\nGorilla\n\nAgents\n\nLilian Wang’s blog on LLM Powered Autonomous Agents\nAman’s blog on Agents\n\nRAG\n\nPaper from NVidia FACTS About Building Retrieval Augmented Generation-based Chatbots\n\n\nLLMs can not reason & plan\n\nLLMs Can’t Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n\n\n\nPost-class\n\nDatasets and Tasks (to train LLMs)\n\nLIMA: less is more for alignment\nInstruction Tuning for Large Language Models: A Survey\nOLMo @ Allen AI - if you are interesting in all aspects of open-source LLM development.\n\nModel Training\n\nPre-training\n\nImproving Language Understanding by Generative Pre-Training\n\nAlignment\n\nLecture 12.2 from AIL821\n\nFine-tuning\n\nPerformance Efficient Fine-Tuning collection\nLecture: PEFT\nLecture: : Quantization and Pruning\nQLoRA: Efficient Finetuning of Quantized LLMs\n\n\nPrompt Optimization\n\nThe Prompt Report\nChain-of-Thought\nTree-of-Thought\nSelf-Reflection\nSelf-Contrast\nThink before you Speak\n\nConstrained Language Generation\n\ncollection\nGuiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\n\nEvaluation\n\nScaling Evaluation of LLMs Yann Bubois, CIS 7000 LLM Course\n\nApplications and Design Patterns\n\nTools\n\n[Lecture 18.2 from AIL821] LLMs and Tools: Function Calling\n\nAgents\n\n[Lecture 18.3 from AIL821] LLMs and Tools: Agentic\nAutoGen repo\nCrewAI repo\nLLM Agent papers collection \nSurvey: The Rise and Potential of Large Language Model Based Agents: A Survey\n\nRAG\n\nRetrieval-Augmented Generation for Large Language Models: A Survey Mar’24\nSearching for Best Practices in Retrieval-Augmented Generation Jul’24\n\n\nLLMs can not reason & plan\n\nLecture 19 from AIL821 Reasoning in LLMs\nGSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\nLearning to reason with LLMs\nSystems-2 Collection Repo\n\n\n\nLLMs and Influence Functions\n\nStudying Large Language Model Generalization with Influence Functions\nDo Influence Functions Work on Large Language Models?\nTextGrad Automatic ‘’Differentiation’’ via Text, paper\n\n\n\nFull Courses\n\nCIS7000 LLM Course @ UPenn by Prof. Mayur Naik. Covers many advanced topics.\nAIL821 LLMs Course @ IIT-D\nDeep Learning For NLP @ UW LING 574, Deep Learning For NLP, Prof. Shane @ UW, Spring’24.\nWalk through the book Building LLMs from Scratch",
    "crumbs": [
      "LLMs-Ops",
      "13A: LLMs Introduction"
    ]
  },
  {
    "objectID": "lectures/w13-l02.html",
    "href": "lectures/w13-l02.html",
    "title": "13B: LLMs Ops",
    "section": "",
    "text": "Materials:\nDate: Thursday, 07-Nov-2024, 3-4.30pm, IST.",
    "crumbs": [
      "LLMs-Ops",
      "13B: LLMs Ops"
    ]
  },
  {
    "objectID": "lectures/w13-l02.html#materials",
    "href": "lectures/w13-l02.html#materials",
    "title": "13B: LLMs Ops",
    "section": "",
    "text": "Pre-work:\n\nLLM Intro\n\n\n\nIn-Class\n\nBuild\n\nBuilding LLMs from Scratch repo\npre-training, fine-tuning, instruction fine-tuning\n\nPrompt Optimization\n\nDSPy, Intro Notebook\n\nStructured outputs in LLMs\n\nwith Instructor\nwith Outlines\nBlog comparing many libraries from structured output generation.\n\nEvaluation\n\nwith DeepEval\nwith ragas\nwith LangSmith\n\nApplications: RAGS\n\nBuilding RAG-based LLM Applications in Production blog, notebook by Gokul Mohandas and Philip Moritz\nAgentic RAG with LangGraph tutorial\n\nApplications: Agents\n\nwith LangGraph tutorials\nblog explaining LangGraph\nMagentic-One\n\n\n\n\nPost-class\n\nTraining with Ray Train\nServing with Ray Serve, vLLM, ollama for serving\nRAGs with Llamaindex, cognita, Langchain\nAgents with LangGraph, AutoGen, CrewAI, LangGraph IDE blog",
    "crumbs": [
      "LLMs-Ops",
      "13B: LLMs Ops"
    ]
  },
  {
    "objectID": "lectures/w13-l03.html",
    "href": "lectures/w13-l03.html",
    "title": "13C: Fullstack LLMs",
    "section": "",
    "text": "Materials:\nDate: tbd",
    "crumbs": [
      "LLMs-Ops",
      "13C: Fullstack LLMs"
    ]
  },
  {
    "objectID": "lectures/w13-l03.html#materials",
    "href": "lectures/w13-l03.html#materials",
    "title": "13C: Fullstack LLMs",
    "section": "",
    "text": "Pre-work:\n\nLLM Intro\nLLM Ops\nXAI Tutorial by Hima Lakkaraju, Julius Adebayo, Sameer Singh\nUQ Tutorial by Balaji Lakshminarayanan\n\n\n\nML Engineering\n\nLLaMA Stack - a full stack LLaMA-centered APIs for inference, safety, agentic system, among others.\nMLFlow LLMs - tool calling, agents, evaluation, RAGs, serving and more\nRay LLMs\nMLFlow Tracing observability for LLMs\nOthers popular stacks LlamaIndex, LangChain\nDeepEval\n\nXAI\n\nXAI @ Harvard, Spring’23, Explainable AI by Prof.Hima Lakkaraju\nPublications by Hima Lakkaraju\nLLMs for XAI\n\nCan Large Language Models Simplify Explainable AI\n\nXAI for LLMs\n\nStudying Large Language Model Generalization with Influence Functions\nDo Influence Functions Work on Large Language Models?\n\n\nUQ\n\nQuantifying Uncertainty in Natural Language Explanations of Large Language Models\nConformal Prediction with Large Language Models for Multi-Choice Question Answering code\n\nSecurity\n\nNeMO Gaurdrails\nLlaMA Gaurd 7B Model, paper",
    "crumbs": [
      "LLMs-Ops",
      "13C: Fullstack LLMs"
    ]
  },
  {
    "objectID": "lectures/w15-l01.html",
    "href": "lectures/w15-l01.html",
    "title": "15A: Meta Learning",
    "section": "",
    "text": "Materials:\nDate: 19th, Nov, 20224, 11.30am IST",
    "crumbs": [
      "Lectures[ML Science]",
      "15A: Meta Learning"
    ]
  },
  {
    "objectID": "lectures/w15-l01.html#materials",
    "href": "lectures/w15-l01.html#materials",
    "title": "15A: Meta Learning",
    "section": "",
    "text": "Pre-work:\n\n\nIn-Class\n\n\nPost-Class",
    "crumbs": [
      "Lectures[ML Science]",
      "15A: Meta Learning"
    ]
  },
  {
    "objectID": "lectures/w16-l01.html",
    "href": "lectures/w16-l01.html",
    "title": "16A: Fairness & Bias",
    "section": "",
    "text": "Materials:\nDate: 26th, Nov, 20224, 11.30am IST",
    "crumbs": [
      "Lectures[ML Science]",
      "16A: Fairness & Bias"
    ]
  },
  {
    "objectID": "lectures/w16-l01.html#materials",
    "href": "lectures/w16-l01.html#materials",
    "title": "16A: Fairness & Bias",
    "section": "",
    "text": "Pre-work:\n\n\nIn-Class\n\nAI Fairness 360 An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias\nnotebook Detecting and mitigating age bias in credit decisions\npaper AI Fairness: from Principles to Practice\n\n\n\nPost-Class\n\ncollection on Fairness in AI\npaper Towards Fairness Certification in AI\npaper Fairness in AI and Its Long-Term Implications on Society\npaper The Pursuit of Fairness in Artificial Intelligence Models: A Survey\npaper Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, And Mitigation Strategies\npaper AI Fairness in Practice\nbook Fair ML Book\npaper Fairness in Deep Learning: A Computational Perspective",
    "crumbs": [
      "Lectures[ML Science]",
      "16A: Fairness & Bias"
    ]
  },
  {
    "objectID": "lectures/w16-l02.html",
    "href": "lectures/w16-l02.html",
    "title": "16B: Machine Unlearning",
    "section": "",
    "text": "Materials:\nDate: 26th, Nov, 20224, 11.30am IST",
    "crumbs": [
      "Lectures[ML Science]",
      "16B: Machine Unlearning"
    ]
  },
  {
    "objectID": "lectures/w16-l02.html#materials",
    "href": "lectures/w16-l02.html#materials",
    "title": "16B: Machine Unlearning",
    "section": "",
    "text": "Pre-work:\n\nInfluence Functions\nSample Hardness\nInfluenciae\nArticle 17 of GDPR, Right to be forgotten\nHW06, implement a simple strategy to implement “right to be forgotten”\n\n\n\nIn-Class\n\nBlog Machine Unlearning, Ken Ziyu Liu.\nTutorial An Introduction to Machine Unlearning\n\n\n\nPost-Class\n\nCollection of Machine Unlearning resources\nExact Algorithm: SISA Machine Unlearning with Sharded, Isolated, Sliced, and Aggregated training\nApproximate Algorithms: Influence Certified Data Removal from ML Models",
    "crumbs": [
      "Lectures[ML Science]",
      "16B: Machine Unlearning"
    ]
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "Homeworks",
    "section": "",
    "text": "Midterm Bonus Problem\nReading/ Coding\nTask:\nDue by\n11.59PM IST, Tuesday, 15th Oct, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-midterm-bonus",
    "href": "homeworks.html#sec-hw-midterm-bonus",
    "title": "Homeworks",
    "section": "",
    "text": "Building a Large Language Model (From Scratch). Book by Sebastian Raschka\nAxiomatic Attribution for Deep Networks. This paper was the precursor to XAI methods like Grad-CAM and Grad-CAM++ and variants.\nExplaining and Harnessing Adversarial Examples(2014). This was the precursor to many adversarial attacks papers and robustness methods in DL and grown into a massive research area with implications in AI security.\ncode Compute gradients of a Deep Network w.r.t to inputs and apply them in XAI and Adversatial Attacks. Video tutorial here\n\n\n\nCreate Midterm-Bonus branch of your private AI-839 repo.\nVerify that you can pre-train 150M GPT-2 small model based on code of Building LLMs from Scratch. Highly recommend reading this book by Sebastian Raschka.\nVerify that you can fine-tune with GPT-2 small model on a binary classification task. Consider the spam/ham example dataset provided in there.\nVerify that you can compute gradients w.r.t to inputs and the weights of the model.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-06",
    "href": "homeworks.html#sec-hw-06",
    "title": "Homeworks",
    "section": "HW-06",
    "text": "HW-06\nReading\n\nRight to Erasure act of GDPR\n\nTask\n\nThis HW focuses on the ability to implement techniques to enforce the “right to erase” policy\nCreate a branch named hw06 of your private repo.\nTreat the model you have built in HW-04 as your baseline, call it Model A\nImagine the first 10 records were to be erased.\nImplement a strategy s.t those 10 records will not be used in any future predictions. Update the API, and Data and Model cards to reflect this change.\nSuggest a method to implement “right to erasure” such that, even all past predictions that were influenced by the 10 records will be re-scored, and pushed.\n\nDue by\n11.59PM IST, Friday, 18th Oct, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-05",
    "href": "homeworks.html#sec-hw-05",
    "title": "Homeworks",
    "section": "HW-05",
    "text": "HW-05\nReading\n\nmlflow Read mlflow documentation\nkedro-mlflow Read kedro-mlflow plug-in documentation\n\nTask\n\nThis HW focuses on Model Comparison and Continuous Improvement\nIt builds on HW03-04. See instructions for HW-03, HW-04.\nCreate a branch named hw05 of your private repo.\nTreat the model you have built in HW-04 as your baseline, call it Model A\nA new tranche of data is shared with you on google drive.\nAsses the performance of Model A on this new tranche data. Is the model performing well on all metrics (i.e data drift)?\nDevelop an alternate model, call it Model B. Run a Hypothesis test to see Model B is better than Model A on new tranche of data shared in your private repo.\nDeploy Model B if the performance is better than Model A on the new tranche of data. Update metadata of the API to reflect the new model version being used to make the predictions.\n\nDue by\n11.59PM IST, Tuesday, 24th Sep, 2024.\nFeedback\n\n\nWrite a separate monitoring pipeline. It is exactly the evaluation pipeline except that it runs on the new ground truth data.\nConditionally, trigger a re-training pipeline, which is again exactly, the training pipeline when the strategy to update the model is retraining on new data or all data including the new delta.\nAdd a new model comparison pipeline\nUsing git-hooks, deploy the model if the performance is satisfactory.\nAutomate as much as possible.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-04",
    "href": "homeworks.html#sec-hw-04",
    "title": "Homeworks",
    "section": "HW-04",
    "text": "HW-04\nReading\n\nmlflow Read mlflow documentation\nkedro-mlflow Read kedro-mlflow plug-in documentation\n\nTask\n\nThis HW focuses on Deployment and Monitoring.\nIt builds on HW03. See instructions for HW-03.\nCreate a branch named hw04 of your private repo.\nMake sure that\n\nDefine the request and response structure of the API. Provide API documentation.\nModel is deployed and API is available via kedro-mlflow plug-in.\nModel usage is logged\n\n\nDue by\n11.59PM IST, Friday, 13th Sep, 2024.\nFeedback\n\n\nSeparate inference pipeline from the training pipeline. Inference pipeline includes the pre-processors, if any, model loading, and model prediction functions. Typically, only models are exposed as inference APIs. This may not work always. Guess why?\nTrain any pre-processors like standard scaler, label encoding after splitting the data into train, test. Not before splitting. Guess why?",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-03",
    "href": "homeworks.html#sec-hw-03",
    "title": "Homeworks",
    "section": "HW-03",
    "text": "HW-03\nReading\n\nEvidently: Read Evidently, an ML & Data Quality and Monitoring tool.\nCh 8 of ML System Design\nBlogs from Great Expectations\n\nTask\n\nThis HW focuses on Data-driven code structure, documentation, code hygiene, testing, and data quality assessment & reporting.\nA Google Drive folder with your github handle is shared. This contains a dataset.\nCreate a branch named hw03 of your private repo.\nSimilar to the Kedro spaceflights tutorial, write your data loading, model training, and testing, pipeline interms of nodes and pipelines.\nMake sure that\n\nAPIs are documented with quartodoc\nCode is linted and formatted (with ruff)\nNodes and Pipelines are Unit tested (with PyTest).\nData Quality and Drift Detection Test suites using Evidently are run and the reports are represented as Plotly nodes in Kedro .\nAutomated test to detect distribution drift of target variable(represented as y in the dataset) between Train and Test splits is run using Evidently. The pipeline fails if distribution drift is detected.\n\n\nFeedback\n\n\nAPI documentation\n\nuse docstrings in code so that they can be parsed and rendered into code documentation automatically. You do not have to write it your self separately.\ncode and documentation should co-exist and should be written at the same time, so that changes in code will and should be reflected in the documentation immediately.\n\nKnow what tests evidently runs, and read the documentation. Running the software is not sufficient.\n\nDue by\n11.59PM IST, Friday, 06th Sep, 2024.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-02",
    "href": "homeworks.html#sec-hw-02",
    "title": "Homeworks",
    "section": "HW-02",
    "text": "HW-02\nReading\n\nData Cards: Purposeful and Transparent Dataset Documentation for Responsible AIpaper blog from Google Research.\n\nTask\n\nDownload the soybean tabular dataset version v1 with dataset id 42 from openML.\nMake sure that a repo is already created (as you would have done in HW-01) with repo name “your first name-ai-839” and git handle dhavala is given read and write access.\n\nCreate a new branch name hw02\nUnder notebooks folder, create a new notebook with markdown and cell tags to implement Data Cards.\nUsing pymfe, include some useful properties of the dataset in the Data Card - - Make sure that any changes in the dataset get reflected automatically in the Data Card. Specifically, if the dataset id 1023 (refers to its version v2), Data Card should reflect any changes in the new version of the data.\nThe CLeAR framework has some really nice goals for a document. A document must be 1) comparable 2) legible 3) actionable and 4) robust to achieve an aspect of AI Transparency. Here we are talking about documenting Data, Models, and AI Systems.\n\nDue by\n11.59PM IST, Friday, 23rd August, 2024.\nFeedback\n\n\nLike Project Cards, identify clear sections. For example, Data Cards should address\n\nwhy was the data collected (if we know it)\nWhen was collected\nWho collected\nWhat was collected. Is there sensitive data? If so, what are they?\n\nDescription/profile of the data\nSample data\nMost datasets on the internet do not have any of these attributes. Imagine, someone gives you a CSV file without the headers? Will that be useful? Put the end user first.\nThe instruction was to use pymfe as it can routinely gather most statistics. Data Cards is not a data scientist’s jupyer notebook. Neatly format. Do not treat Data Cards as notebook with code blocks and output cells. It is data-driven document. It is neither a document with static content or jupyter notebook.\nUse modern publishing tools like quarto which erase the boundary between word processors like word and EDA notebooks. Do not reinvent the wheel and render your own markdown strings from notebook cell outouts.\nFocus is on data-driven documentation.",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "homeworks.html#sec-hw-01",
    "href": "homeworks.html#sec-hw-01",
    "title": "Homeworks",
    "section": "HW-01",
    "text": "HW-01\nReading\n\nChip Huyen’s Lecture Notes on ML in Production. Intro here\nPhase Zero of MLOps from ml-ops.org blogs. Very useful and nicely done\nTowards CRISP-ML(Q): A Machine Learning Process Model with QA Methodology\nProject Canvas from Gokul Mohandas’s course\nAI Canvas\n\nTask\n\nPick your favorite problem. Complete the information required in the project card template\nCreate a new kedro project with repo name “your first name-ai-839”. Documentation here\nPush it to github. Give read read/write acces to git handle dhavala\nCreate a branch “hw01”. Under notebooks folder, copy project card template\nComplete the Project Card, commit and push the notebook to your github (remote) repo.\n\nDue by\n11.59PM IST, Tuesday, 13th August, 2024.\nFeedback\n\n\nComplete all sections - even if the problem is hypothetical.\nIn reality, authoring Project Cards is an iterative and collaborative exercise. But in this HW, you are forced to think through. Information may be inaccurate but should not be incomplete. Recall that one of the objective of the CLeAR framework is to make documentation comparable. If some sections of this standard template are missing, these docs can not be compared.\nBusiness View\n\nwas the hard part to get it right. Often, models, ML metrics, features of the ML model crept into this section. Very little ML will go into this section.\n\nBusiness View: Background\n\nDo not describe the solution. Describe the situation of the user. It is like the backdrop. It should describe the “where” of your solution. Once the why section is read through, it should resonate well. Marketing & Sales teams, Product and User Research teams should be consulted (or lead by them).\n\nIn the Business View: Problem section, often solution is outlined. Don’t commit to a solution just as yet. Describe the problem - what to be solved, not how it is being solved. It is not the ML problem yet. Think of this way. Even of the solution does not involve ML, it could still have been a good problem to solve.\nBusiness View: Customer\n\nKeep it as specific as possible. Simply put, every additional adjective, and qualifier will identify the customer better.\n\nBusiness View: Value Proposition\n\nIt is largely about reduction in something or improvement of something by a certain quantity.\n\n\nBusiness View: Product\n\nIt is not about the features. Features is not equal to experience. Focus on the experience, journey, workflow of the user. Think of user stories. One should almost be able to visualize how they will use your solution and get benefit from.\n\nBusiness View: Objectives\n\nObjectives are not wishlists. An objective should at least have a deliverable (outcome) and a timeline.\n\nBusiness View: Risks & Challenges\n\nProbably, we should can keep ML Risks separate. Here, it is about Viability, Adoption and Scalability of the Solution and the assumptions went into it.\nData challenges are real and most pressing.\nIs the eco-system ready to handle ML responses? Can the users handle wrong information? Is there a redressal/ recourse mechanism?\nThe user has knowledge of the application? In other words, can user review the information provided by the application or naively believes the results?\nThis is the most paradoxical issue with ML. Are you solving the problem from the past? The historical data is about the past. But do we expect the world to live that way? Is ML necessary, could it have been solved differently?",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Major Project\nTask\nNote:\nDue:\nProposal Submission:\n- 11.59PM IST, Friday, 20th Oct, 2024.\nPresentations:\n- 26th and 29th, Nov, 2024, in-class.\nFinal Submission:\n- 11.59PM IST, Friday, 29th Nov, 2024.",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects.html#sec-hw-major",
    "href": "projects.html#sec-hw-major",
    "title": "Projects",
    "section": "",
    "text": "This major project focuses on baking all the software engineering best practices, MLOps lifecycle and holistic ML into one code base. All the objectives of the Minor Project should be there. In addition,\n\nat dataset (aggregate) level, models are\n\nexplainable\ncalibrated\nassessed for robustness\n\nat inference time, for every instance, the following are available\n\nconformal predictions\nexplanations\ntrust scores\n\n\n\n\n\nYou can choose any problem or build on the Tabular Data provided in the class. Above must be available on major-project branch of your course private repo.\nSubmit your proposal no later than October 20th, 2024, Friday, 13.00pm IST. In the repo, create a README.md file at the root.\nIn addition to the code, you have present the project on 26th-29th November, in-class.",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects.html#sec-hw-minor",
    "href": "projects.html#sec-hw-minor",
    "title": "Projects",
    "section": "Minor Project",
    "text": "Minor Project\nTask\n\nThis minor (mini) project focuses on baking all the software engineering best practices and MLOps lifecycle into one code base\nCreate a branch named minor-project of your private repo.\nFor the dataset including all Tranche shared on your google drive, show that, your Models, Data, and Pipelines are\n\nProject Cards, Data Cards, Model Cards, MLOps Cards are populated and are data driven.\nCode is modular, linted, and tested\nCI/CD hooks are implemented\nModel is deployed and available via an API.\nInference can be scaled\nModel is monitored, and a new model is deployed when an opportunity or a need to redeploy is detected.\nRight to be forgotten is enabled.\n\nOne way to think about it is, it is is culmination of HWs up until now.\n\nDue by\n11.59PM IST, Friday, 25th Oct, 2024.",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "GIT",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#sec-tut-git",
    "href": "tutorials.html#sec-tut-git",
    "title": "Tutorials",
    "section": "",
    "text": "[video] Git Tutorial for Beginners by Code-With-Mosh.\n[Paid Course] Ultimate Git by Code-with-Mosh is very good. Covers, additional topics like Collaboration, Advanced Branching strategies, etc.\n[video] Git and Github for Beginners - Crash Course from freeCodeCamp.org\n[book] Pro Git\n[web] Learn Git Branching - an interactive way to learn git branching",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#sec-tut-git-actions",
    "href": "tutorials.html#sec-tut-git-actions",
    "title": "Tutorials",
    "section": "Git Actions",
    "text": "Git Actions\n\n[Documentation] Quick Start Github offocial documentation\n[video] Git Actions for CI/CD",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#sec-tut-docker",
    "href": "tutorials.html#sec-tut-docker",
    "title": "Tutorials",
    "section": "Docker",
    "text": "Docker\n\n[book] The Docker Handbook Book\n[Paid Course] The Ultimate Docker Course by Code-with-Mosh is very good.\n[tutotial] Please-Contain-Yourself. Liked the title :)",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Ops, Strategy Planning for AI by Soma S. Dhavala, given at DFL’s Responsible AI Impact Lab (RAIL) Fellowship program.\nModel Quantization, Pruning, and Compression, Dr. Srinivas Rana\nCausal ML, Dr. Amit Sharma\nAI Security, Manojkumar Parmar, notebooks on Text Attacks, Adversarial Attacks",
    "crumbs": [
      "Talks"
    ]
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "ML Documentation",
    "section": "",
    "text": "Model Cards\nGoogle published Model Cards for Model Reporting to improve transparency in model reporting. Idea is very similar to how information related to the nutritional content is published on the packaging. See their blog for details.",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#business-view",
    "href": "documentation.html#business-view",
    "title": "ML Documentation",
    "section": "Business View",
    "text": "Business View\nIt focusses on the following questions:\n\nWhat is the problem being solved?\nWho is the customer?\nWhy it needs to be solved?\nHow does the solution like - a mental or a conceptual model or a mock of the product?\nWhat objectives does it achieve?\nWhat are the risks and challenges?",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#ml-view",
    "href": "documentation.html#ml-view",
    "title": "ML Documentation",
    "section": "ML view",
    "text": "ML view\nIt is bit more in-depth focussing on the execution will have the following questions:\n\nWhat is the prediction problem?\nHow the objective will be measured?\nHow will it be tested?\nData: What kind, how and how much and what for?\nWhat is the roadmap/plan?\nWhat resources are needed - both human, compute and admin?",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#what-is-the-solution",
    "href": "documentation.html#what-is-the-solution",
    "title": "ML Documentation",
    "section": "What is the solution",
    "text": "What is the solution\n\nSurprisingly simple. Just tag a notebook cell - who is it for?\nAnd take benefit of modern document publishing tools and workflows like Quarto, GitHub, GitHub actions",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "documentation.html#core-tenets",
    "href": "documentation.html#core-tenets",
    "title": "ML Documentation",
    "section": "Core tenets",
    "text": "Core tenets\n\nOne content - many views\nData + Code + Content &gt; should drive the documentation (format, style, purpose)\nEach stakeholder’s documentation need is just a view or a content rendering problem\nPublishing documentation = Publishing code\nUse the same tools and mental models both for code and documents\nSingle source of truth for any derived document\nFix in only one place and only once.\nPhysical and mental distance between Documentation and Code should be (close to) ZERO\nSet up once and automate subsequently\nAutomate the publishing process\nNo human oversight should be necessary for process compliance\nCommit code + documentation content &gt; rendering must be automated\n\nAbove points are put in a Project Card Template which is a Jupyter notebook.",
    "crumbs": [
      "ML Documentation"
    ]
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Book",
    "section": "",
    "text": "Content",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Book</span>"
    ]
  },
  {
    "objectID": "book.html#references",
    "href": "book.html#references",
    "title": "Book",
    "section": "References",
    "text": "References\n\n[book] ML Engineering, Andiry Burkov, 2019, LeanPub\n[book] Effective Data Science Infrastructure, Vile Tuulos, 2023, Manning\n[book] ML System Design, Chip Huyen, 2023, O’Reilly\n[course] CS329S @ Stanford: ML Systems Design, Chip Huyen, 2022\n[course] MLOps, Chip Huyen, 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Book</span>"
    ]
  },
  {
    "objectID": "chapters/c00.html",
    "href": "chapters/c00.html",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Materials:\nDate: Thursday, 01-Aug-2024.",
    "crumbs": [
      "ML Engg",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "chapters/c00.html#materials",
    "href": "chapters/c00.html#materials",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\n\n\n\nIn-Class\n\nWe will go over W01-L01-Deck based on this from Chip Huyen’s CS329S: ML System Design Fall’22 @ Stanford\nSoftware 1.0 vs Software 2.0. See this Figure\n\n\n\nPost-class:\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper\nCh1-2 of AB, Ch1 of VT, Ch1 of CH1",
    "crumbs": [
      "ML Engg",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "mlops.html",
    "href": "mlops.html",
    "title": "Book",
    "section": "",
    "text": "Content",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Book</span>"
    ]
  },
  {
    "objectID": "mlops.html#references",
    "href": "mlops.html#references",
    "title": "Book",
    "section": "References",
    "text": "References\n\n[book] ML Engineering, Andiry Burkov, 2019, LeanPub\n[book] Effective Data Science Infrastructure, Vile Tuulos, 2023, Manning\n[book] ML System Design, Chip Huyen, 2023, O’Reilly\n[course] CS329S @ Stanford: ML Systems Design, Chip Huyen, 2022\n[course] MLOps, Chip Huyen, 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Book</span>"
    ]
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "Updates",
    "section": "",
    "text": "Overview\nPrereqs\nPart-1: Essentials (ML Engineering)\nPart-2: Full Stack MLOps",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "news.html#announcements",
    "href": "news.html#announcements",
    "title": "Updates",
    "section": "",
    "text": "[01-January-2025] Material launched based on AI-839 taught at IIIT-B in the Fall of 2024.",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "news.html#overview",
    "href": "news.html#overview",
    "title": "Updates",
    "section": "",
    "text": "See preface for why MLOps and the approach and outlook taken here.\nSee the Full Stack MLOps page for recent information on Lecture Notes, Homeworks, Projects, etc..\n\n\n\nExposure and skill in data handling, building models in Python, PyTorch\nExposure and skill in developing code using Python, Git, IDEs like VS Code\nA foundation course in Machine Learning, Deep Learning, Data Modeling, working with (Big) Data\n\n\n\nTopics\n\nMLOps motivation, need\nBasic principles and MLOps with Open Source Software\n\nLearning Outcomes: students will be able to\n\nDeploy models with logging, documentation, unit tests, and APIs\nUnderstand a conceptual framework to approach MLOps holistically\n\n\n\n\nTopics\n\nHolistic understanding of ML development, beyond chasing typical performance metric\n\nLearning Outcomes: students will be able to\n\ndeploy models, observe their performance, make improvements, redeploy them.\nensure that the ML pipeline is reproducible.\nincorporate principles from Responsible AI and build ML systems which can consist of many models and tools.\nframe, discover, develop, deploy, monitor, improve, re-deploy and maintain an ML Application",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Full Stack MLOps",
    "section": "References",
    "text": "References\n\n[book] ML Engineering, Andiry Burkov, 2019, LeanPub\n[book] Effective Data Science Infrastructure, Vile Tuulos, 2023, Manning\n[book] ML System Design, Chip Huyen, 2023, O’Reilly\n[course] CS329S @ Stanford: ML Systems Design, Chip Huyen, 2022\n[course] MLOps, Gokul Mohandas, 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full Stack MLOps</span>"
    ]
  },
  {
    "objectID": "chapters/00-pods.html",
    "href": "chapters/00-pods.html",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Materials:\nDate: Thursday, 02-Jan-2025.",
    "crumbs": [
      "Introduction",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "chapters/00-pods.html#materials",
    "href": "chapters/00-pods.html#materials",
    "title": "01A: Grounding MLOps",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nRead the preface of this book\n\n\n\nThis\n\nWe will go over W01-L01-Deck based on this from Chip Huyen’s CS329S: ML System Design Fall’22 @ Stanford\nSoftware 1.0 vs Software 2.0. See this Figure\n\n\n\nPost-class:\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper\nCh1-2 of AB, Ch1 of VT, Ch1 of CH1",
    "crumbs": [
      "Introduction",
      "01A: Grounding MLOps"
    ]
  },
  {
    "objectID": "ch00/oss-ml.html",
    "href": "ch00/oss-ml.html",
    "title": "00A: OSS for ML",
    "section": "",
    "text": "Date: Thursday, 02-Jan-2025\nAuthor: Soma S Dhavala\nVersion: 0.1.0\n\nPre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nRead the preface of this book\n\n\n\nLecture\nWe will go over the Deck. Video [tbd]\nDownload PDF file.\n\n\nNotes\nWe like to address the question Is developing ML software (and models) similar to developing (traditional) software?. If not, what makes it different? What type of software tools, frameworks, processes are needed - both from Engineering (technology) and Science (techniques) point of view?\n\nSoftware 2.0 - the Chief of Pain!\n\nAndrej Karpathy is said to have coined the term Software 2.0 to refer modern day ML. If any thing, Software 2.0 is a different beast compared to Software 1.0 (the classical software).\n\nSoftware 2.0 is a nascent field as far as Enterprise-level maturity is concerned unlike Software 1.0 that has matured into a stable industry over a period of decades. Exceptions exists of course! The MAANGs and FAANGs of the world have figured out to tame this beast into a profit-making technology.\nThe development is highly non-linear and often the problem to solve is not clear as it is data and environment dependent. Part of the job is to discover what problem to solve (problem formulation).\nFunctional requirements are codified in the data, and not in an SRD doc written by a Product Owner.\nBehavior (expected functionality) is realized in opaque bytes, and not written or programmed in formal languages by developers (human beings).\n\nIt can produce mistakes and errors with probability greater than zero.\nThis provably-wrong-ness is the anti-thesis of determinism and reliability that is expected in any (semi) autonomous systems.\n\n\nAnything changes, everything changes.\n\nAI has has no common sense unlike us humans, and sees only the world it is trained on. Any variable in the equation, data, model, application context & situation change, the AI may not longer work as expected. This makes deploying and maintaining AI complicated.\n\nThe Two Language Problem\n\n\nThe skills sets to ideate, formulate, develop, deploy, monitor, improve, scale, and maintain are varied. When the programming languages are different between the Dev and Prod environments, it is referred to as the two-language/env problem.\nA single person might not have necessary breadth and depth to offer the build quality. And in some cases, POC stack & environment can be different from the Prod stack.\nDifferences in the Dev and Prod environments can further exacerbate an already messy situation.\nModern frameworks like MetaFlow address this key issue of idempotency. But again, no single tool/library/framework may be sufficient as no one size fits all. The empahsis and priority depend on type of the problem and the type of the organization. It can lead to over-engineering and premature optimziation otherwise. Read this Google article on MLOps levels.\n\n\nModel ing - not Models. System - not Models.\n\n\nAs mentioned in the preface, it is difficult to not worry about models and only about models. Celebrities like Andrew Ng have to call out for Data-Centric AI to regain focus on data and its quality. Otherwise, expect to see garbage-in, garbage-out\nBut it is not just about data, it is also about the whole modeling process - right from data collection to post-deployment monitoring and improvement and everything in between. Model is all but one part of this data flow.\nDeveloping production-worthy models require aspects like testing, documentation, code quality, readability (of code), knowledge transfer (from one team member to another), which are often ignored in a model-centric workflow and mindset.\n\n\nBeyond Performance\n\n\nIt is an unusual sight if a developer is not asked of model performance and whether the performance can be improved further. This is a symptom of model-centric thinking.\nBut real world application may have different priorities. For example, a model that is explainable and interpretable is better than performant but black-box model.\nMany other aspects like Robustness, Security, Expandability are important.\nMay be the model should communicate the uncertainty in the predictions so as to make the predictions in particular and application in general trustworthy. So, providing prediction intervals as opposed to communicating only point predictions is important.\nInputs and Outputs should be properly validated. Since AI is does have common-sense, it resolved any prediction about the input into concepts it has seen in the dataset. For example, if the model is trained to classify an image as cat or dog, the prediction will always be either dog or a cat. Therefore, it may be much better if the model can abstain from making predictions when it is unsure of the inputs and outputs.\n\nBy excessively and obsessively worrying about and optimizing for model improvements, while fixing the data, is one of leading causes of premature and untimely death of AI initiatives. Battling this intellectual inertial is extremely hard.\nAll of the above make Software 2.0 hard to formulate, develop, verify, explain, debug, explain, control, improve, and maintain. At the same time, ensuring that the models are explainable, trustworthy, robust, secure, reliable, is a double whammy. Therefore, all software systems, tools, and process must address this core issue.\nThis incurs lot of tech debt. And a git template that integrates many tools with specific responsibilities is needed. Goal is to support as many quality attribute as possible by delegating them to tools as much as possible. We refer to such a stack as ML Pod. Such a git template for supporting image-based workflows is under development here. Read the documentation here.\n\n\nFurther Reading\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper",
    "crumbs": [
      "Introduction",
      "00A: OSS for ML"
    ]
  },
  {
    "objectID": "ch00/oss-ml.html#materials",
    "href": "ch00/oss-ml.html#materials",
    "title": "00A: OSS for ML",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nRead the preface of this book\n\n\n\nLecture\nWe will go over the Deck. Video [tbd]\n\n\nNotes\n\nSoftware 2.0 - the Chief of Pain!\n\nAndrej Karpathy is said to have coined the term Software 2.0 to refer modern day ML. If any thing, Software 2.0 is a different beast compared to Software 1.0 (the classical software).\n\nSoftware 2.0 is a nascent field as far as Enterprise-level maturity is concerned unlike Software 1.0 that has matured into a stable industry over a period of decades. Exceptions exists of course! The MAANGs and FAANGs of the world have figured out to tame this beast into a profit-making technology.\nThe development is highly non-linear and often the problem to solve is not clear as it is data and environment dependent. Part of the job is to discover what problem to solve (problem formulation).\nFunctional requirements are codified in the data, and not in an SRD doc written by a Product Owner.\nBehavior (expected functionality) is realized in opaque bytes, and not written or programmed in formal languages by developers (human beings).\n\nIt can produce mistakes and errors with probability greater than zero.\nIf anything, this provably-wrong-ness is the anti-thesis of determinism and reliability that is expected in any (semi) autonomous systems.\n\n\nAnything changes, everything changes\n\nAll of the above make Software 2.0 hard to formulate, develop, verify, explain, debug, explain, control, improve, and maintain. Therefore, all software systems, tools, and process must address this core issue.\n\nModeling, not Model. System not\n\n\nThe skills sets to ideate, formulate, develop, deploy, monitor, improve, scale, and maintain are varied.\n\n\nThe Two Language Problem\n\n\nThe skills sets to ideate, formulate, develop, deploy, monitor, improve, scale, and maintain are varied.\nA single person might not have necessary breadth and depth to offer the build quality. And in some cases, POC stack & environment can be different from the Prod stack. Such differences in the Dev and Prod environments can further exacerbate an already messy situation. But how to avoid this two-language/env problem? Modern frameworks like MetaFlow address this key issue. But again, no single tool/library/framework may be sufficient as no one size fits all. The empahsis and priority depend on type of the problem and the type of the organization. It can lead to over-engineering and premature optimziation otherwise. Read this Google article on MLOps levels.\n\n\n\nFurther Reading\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper\nCh1-2 of AB, Ch1 of VT, Ch1 of CH1\n\n\n\nExercises",
    "crumbs": [
      "Introduction",
      "00A: OSS for ML"
    ]
  }
]