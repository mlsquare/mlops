[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Full Stack MLOps",
    "section": "",
    "text": "Content",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full Stack MLOps</span>"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Full Stack MLOps",
    "section": "References",
    "text": "References\n\n[book] ML Engineering, Andiry Burkov, 2019, LeanPub\n[book] Effective Data Science Infrastructure, Vile Tuulos, 2023, Manning\n[book] ML System Design, Chip Huyen, 2023, O’Reilly\n[course] CS329S @ Stanford: ML Systems Design, Chip Huyen, 2022\n[course] MLOps, Gokul Mohandas, 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full Stack MLOps</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why MLOps?\nFirst and foremost, let us swallow the bitter pill - ML is just a piece of technology like any other to solve a business problem for which somebody needs to pay for. It means that unless people use it and pay for this technology (models), no matter how sophisticated and cool do they sound, they byte the dust like most ML initiatives do.\nNot just that - industry needs and loves determinism, accountability and reliability, which is the antithesis on which ML lives and thrives. MLOps is about addressing this inherent design challenge using a combination of tools and processes. Without which one has to appeal to luck to achieve the desirable. And we know from experience that luck favors the prepared.\nThis collection of notes on MLOps is about that preparation to manage the entire life cycle of an ML product, holistically and comprehensively. That is, for example, the ML system must be performant, explainable, reliable, fair and transparent, responsive and responsible, controllable, cost effective, collaborative, and many others, all which will discussed in detail later in the course.\n\n\nWhy A course on MLOps?\n\nUnderestimation of Technical Debt\n\nDeveloping ML systems involves significant technical complexities that are often underestimated, chiefly arising out of model-centric pedagogy.\nUnlike traditional software development, ML systems require continuous monitoring and iteration of the situated environment\n\nFundamental Differences from Traditional Software Development\n\nTraditional software development assumes a stable environment and known requirements.\nML systems, however, must adapt to changing environments and data, making it impossible to assume a perfect product at deployment.\n\nUncertain Data Environment\n\nAI systems inherently make mistakes and must be designed to handle continuous change and evolution.\nA good model in the lab does not guarantee performance in production, necessitating robust MLOps practices.\nThe deployment and monitoring of ML systems differ fundamentally from traditional software.\nContinuous monitoring and iteration are essential for maintaining model performance and relevance.\n\nOperationalizing ML\n\nOperationalizing ML is crucial for deriving value from AI systems, aligning them with business objectives and real-world applications. This can be a non-trivial activity, if not impossible in some cases.\nThis mindset is essential for understanding the practical aspects of ML deployment and maintenance.\n\n\nA course on MLOps exposes the developer or system designer to this varying challenges in the life cycle of ML (which is perpetual). This is entirely different from how one could solve an ML problem in academic settings where, more often than not, model novelty is incentivized over utility.\n\n\nWhy THIS course on MLOps?\nThere are many great resources available on the web in terms of books, blogs, courses, and tool documentation. Why yet another one? A legit question.\nAt a philosophical level - there are two objectives:\n\nFight the SOTA syndrome:\nLet me explain.\nThanks to chatGPT, it all seems to be about LLMs - building new models, spinning new shiny applications built around those LLMs both open and closed included. These new shiny objects exacerbated an already sick situation - a situation where building models (model.train) showing 0.01% (or even less) increase in accuracy is chased and cherished – all about State-of-the-Art (SOTA).\nWhile we must work hard and smart to move the needle, and it might be satisfying and fulfilling intellectually – unfortunately, it (chasing SOTA) is neither necessary nor sufficient to build reliable systems consisting of at least one ML component. There is lot more one has to consider in building and managing such systems.\nPrioritizes utility over novelty:\nAdmittedly, it is a boring job to be done. However, contrary to the prejudice, in the due course, you will learn that, indeed, solving for utility and overcoming the challenges along the way, is a rewarding journey in itself. Give no cult status to SOTA.\n\nMore concretely, there are some limitations of current MLOps Resources - they are notably:\n\nPractitioner-Focused Literature\n\nMost current MLOps books and resources are aimed at practitioners.\nThese resources focus heavily on tools and practices without structured assessment components.\nOften they anchor on specific framework/tool set.\n\nNeed for Academic Assessment\n\nIn academia, assessments are a crucial part of the learning process.\nThis material should not only teach concepts but also include tests to evaluate understanding.\nTesting is fundamental to learning and ensures that students grasp and can apply MLOps concepts effectively.\n\nComprehensive Curriculum\n\nThe course should cover not only MLOps but also ML system design and the operationalization of models.\nIntegrating these elements ensures a holistic understanding of both the development and deployment aspects of ML projects.\n\n\nSo the chief difference is - most of the resources are meant for practitioners. But this set of notes is meant both for students who can learn the tools and processes, and apply them and also for practitioners (who can learn the principles). But some of the challenges are more systemic and pervasive outlined below:\n\nVariability in MLOps Practices\n\nMLOps processes and practices vary significantly between individuals and organizations.\nUnlike DevOps in the software industry, which has become a mainstream discipline with standardized practices, MLOps is still relatively young.\n\nAbsence of Standard Vocabulary\n\nThere is no non-denominational vocabulary in MLOps, leading to inconsistencies in terminology and practices.\nStandardizing terminology and processes is crucial for effective communication and collaboration within and between organizations.\n\nKnowledge Diffusion and Skill Gap\n\nTraditional academic institutions typically propagate certain ideas and knowledge that get absorbed and amplified in the industry. But the reverse diffusion is often delayed.\nThere can be a lag between what academia offers and what the industry needs, especially in rapidly evolving fields like ML.\nset of templates (of code bases and also of materialized principles) that anybody can use in the Industry which improves the overall employee productivity of the employee\n\nNo Institutionalized MLOps Thinking\n\nInstitutionalizing MLOps in academic programs ensures that students are trained in industry-relevant skills from the start.\nThis approach can help standardize the quality and relevance of ML education, making graduates industry-ready.\nIntroduce a conceptual framework of models, actors and actions involved and a vocabulary to describe the complex ML dev cycle\nProvide a holistic view of ML - going beyond chasing performance metrics\npractice the principle theory-with-code and code-with-theory so that every principle is practicable, and every practice has a principle\ndevelop good (conceptual) models and principles which industry can adopt\n\n\n\n\nBenefits for Students\n\nIndustry Readiness\n\nInstead of learning these skills during internships or on the job, students will acquire them as part of their academic experience.\nThis prepares students to be industry-ready upon graduation, reducing the training burden on employers.\n\nHolistic Approach to Machine Learning\n\nThe course promotes a holistic view of ML, integrating model-centric and data-centric approaches.\nStudents will learn to consider all aspects of ML, including data quality, model robustness, and system integration.\n\nResponsible AI\n\nThe curriculum will cover aspects of responsible AI, ensuring that students are aware of ethical considerations and best practices in AI deployment.\nThis includes understanding biases, fairness, transparency, and accountability in ML models.\n\nComprehensive Skill Set\n\nStudents will gain a broad set of skills, from ML system design to operationalizing models.\nThis comprehensive skill set ensures that graduates are well-prepared to handle the full ML lifecycle in professional settings.\n\n\n\n\nBenefits for Academic Institutions\n\nPioneering Course Offering in India\n\nThis course is likely the first of its kind being offered in India, positioning the institution as a leader in ML education.\nBuilding on existing curricula such as Introduction to Machine Learning, Deep Learning, and Introduction to DevOps.\n\nIndustry-Relevant Education\n\nBy incorporating industry perspectives, including guest lectures and hands-on projects, the course aligns academic training with real-world needs.\nFold industry needs into the curriculum: students often gain experience in developing models and ML system design through internships, and this is transactional by design. This experiential knowledge has to be scaled with backward integration into the curriculum\nThis practitioner-oriented approach ensures students gain practical experience.\n\nEnhanced Employability\n\nAcademic institutions benefit by producing graduates who are immediately employable.\nStrong industry partnerships and placement opportunities can be developed, enhancing the institution’s reputation and appeal to prospective students.\nEquip students with essential skills: in ML lifecycle management, including deployment, monitoring, and automation\nPrepares students for roles such as ML engineer, MLOps engineer, and data scientist with operations expertise\nFamiliarize students with MLOps tools and platforms (e.g., Kubeflow, MLflow, TFX).\nEnhance the overall learning experience: By providing practical, hands-on experience with industry-relevant tools and practices\n\nCater to new market: As there is growing demand for MLOps skills in the industry due to the increasing adoption of AI and ML technologies. Organizations are looking for professionals who can manage the end-to-end ML lifecycle\n\n\n\nBenefits for Industry\n\nReducing Ad-Hoc Practices\n\nThe course aims to reduce the ad-hoc nature of MLOps practices and bring consistency to tooling choices.\nSimilar to how standardized practices in software development (e.g., Java build tools, POM) have streamlined processes, this course seeks to standardize MLOps practices.\n\nBuilding a Talent Pool\n\nThe industry will benefit from a pool of talent that is job-ready, reducing the onboarding time and training costs.\nGraduates will be proficient in MLOps practices, making them productive and profitable employees from the start.\n\nConceptual Frameworks for MLOps\n\nWhile the tools and techniques for implementing MLOps may vary, the course will provide students with robust frameworks and methodologies.\nThese frameworks will help students understand and apply MLOps principles in various contexts and adapt to new technologies as they emerge.\n\n\n\nThis course aims to introduce concepts that will stand the test of time, despite the rapid evolution of tools and techniques.\n\nBy focusing on foundational principles, the course provides a framework for thinking about MLOps that remains relevant as the field evolves.\n\n\n\nConsistency and Knowledge Transfer\n\nStandardized MLOps training ensures that professionals can move between projects with ease, facilitating better knowledge transfer and collaboration.\nThis reduces the time and effort needed to get new hires up to speed on MLOps practices within the organization.\n\n\n\n\nStyle\nContent will be presented in the form of take-away points, rather than main take-aways embedded in long winding paragraphs. Nuances etc will be added in the due course of time or video recordings will be made available.\n\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "Updates",
    "section": "",
    "text": "Overview\nPrereqs\nPart-1: Essentials (ML Engineering)\nPart-2: Full Stack MLOps",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "news.html#overview",
    "href": "news.html#overview",
    "title": "Updates",
    "section": "",
    "text": "See preface for why MLOps and the approach and outlook taken here.\nSee the Full Stack MLOps page for recent information on Lecture Notes, Homeworks, Projects, etc..\n\n\n\nExposure and skill in data handling, building models in Python, PyTorch\nExposure and skill in developing code using Python, Git, IDEs like VS Code\nA foundation course in Machine Learning, Deep Learning, Data Modeling, working with (Big) Data\n\n\n\nTopics\n\nMLOps motivation, need\nBasic principles and MLOps with Open Source Software\n\nLearning Outcomes: students will be able to\n\nDeploy models with logging, documentation, unit tests, and APIs\nUnderstand a conceptual framework to approach MLOps holistically\n\n\n\n\nTopics\n\nHolistic understanding of ML development, beyond chasing typical performance metric\n\nLearning Outcomes: students will be able to\n\ndeploy models, observe their performance, make improvements, redeploy them.\nensure that the ML pipeline is reproducible.\nincorporate principles from Responsible AI and build ML systems which can consist of many models and tools.\nframe, discover, develop, deploy, monitor, improve, re-deploy and maintain an ML Application",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "ch00/oss-ml.html",
    "href": "ch00/oss-ml.html",
    "title": "00A: OSS for ML",
    "section": "",
    "text": "Date: Thursday, 02-Jan-2025\nAuthor: Soma S Dhavala\nVersion: 0.1.0\n\nPre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nRead the preface of this book\n\n\n\nLecture\nWe will go over the Deck. Video [tbd]\nDownload PDF file.\n\n\nNotes\nWe like to address the question Is developing ML software (and models) similar to developing (traditional) software?. If not, what makes it different? What type of software tools, frameworks, processes are needed - both from Engineering (technology) and Science (techniques) point of view?\n\nSoftware 2.0 - the Chief of Pain!\n\nAndrej Karpathy is said to have coined the term Software 2.0 to refer modern day ML. If any thing, Software 2.0 is a different beast compared to Software 1.0 (the classical software).\n\nSoftware 2.0 is a nascent field as far as Enterprise-level maturity is concerned unlike Software 1.0 that has matured into a stable industry over a period of decades. Exceptions exists of course! The MAANGs and FAANGs of the world have figured out to tame this beast into a profit-making technology.\nThe development is highly non-linear and often the problem to solve is not clear as it is data and environment dependent. Part of the job is to discover what problem to solve (problem formulation).\nFunctional requirements are codified in the data, and not in an SRD doc written by a Product Owner.\nBehavior (expected functionality) is realized in opaque bytes, and not written or programmed in formal languages by developers (human beings).\n\nIt can produce mistakes and errors with probability greater than zero.\nThis provably-wrong-ness is the anti-thesis of determinism and reliability that is expected in any (semi) autonomous systems.\n\n\nAnything changes, everything changes.\n\nAI has has no common sense unlike us humans, and sees only the world it is trained on. Any variable in the equation, data, model, application context & situation change, the AI may not longer work as expected. This makes deploying and maintaining AI complicated.\n\nThe Two Language Problem\n\n\nThe skills sets to ideate, formulate, develop, deploy, monitor, improve, scale, and maintain are varied. When the programming languages are different between the Dev and Prod environments, it is referred to as the two-language/env problem.\nA single person might not have necessary breadth and depth to offer the build quality. And in some cases, POC stack & environment can be different from the Prod stack.\nDifferences in the Dev and Prod environments can further exacerbate an already messy situation.\nModern frameworks like MetaFlow address this key issue of idempotency. But again, no single tool/library/framework may be sufficient as no one size fits all. The empahsis and priority depend on type of the problem and the type of the organization. It can lead to over-engineering and premature optimziation otherwise. Read this Google article on MLOps levels.\n\n\nModel ing - not Models. System - not Models.\n\n\nAs mentioned in the preface, it is difficult to not worry about models and only about models. Celebrities like Andrew Ng have to call out for Data-Centric AI to regain focus on data and its quality. Otherwise, expect to see garbage-in, garbage-out\nBut it is not just about data, it is also about the whole modeling process - right from data collection to post-deployment monitoring and improvement and everything in between. Model is all but one part of this data flow.\nDeveloping production-worthy models require aspects like testing, documentation, code quality, readability (of code), knowledge transfer (from one team member to another), which are often ignored in a model-centric workflow and mindset.\n\n\nBeyond Performance\n\n\nIt is an unusual sight if a developer is not asked of model performance and whether the performance can be improved further. This is a symptom of model-centric thinking.\nBut real world application may have different priorities. For example, a model that is explainable and interpretable is better than performant but black-box model.\nMany other aspects like Robustness, Security, Expandability are important.\nMay be the model should communicate the uncertainty in the predictions so as to make the predictions in particular and application in general trustworthy. So, providing prediction intervals as opposed to communicating only point predictions is important.\nInputs and Outputs should be properly validated. Since AI is does have common-sense, it resolved any prediction about the input into concepts it has seen in the dataset. For example, if the model is trained to classify an image as cat or dog, the prediction will always be either dog or a cat. Therefore, it may be much better if the model can abstain from making predictions when it is unsure of the inputs and outputs.\n\nBy excessively and obsessively worrying about and optimizing for model improvements, while fixing the data, is one of leading causes of premature and untimely death of AI initiatives. Battling this intellectual inertial is extremely hard.\nAll of the above make Software 2.0 hard to formulate, develop, verify, explain, debug, explain, control, improve, and maintain. At the same time, ensuring that the models are explainable, trustworthy, robust, secure, reliable, is a double whammy. Therefore, all software systems, tools, and process must address this core issue.\nThis incurs lot of tech debt. And a git template that integrates many tools with specific responsibilities is needed. Goal is to support as many quality attribute as possible by delegating them to tools as much as possible. We refer to such a stack as ML Pod. Such a git template for supporting image-based workflows is under development here. Read the documentation here.\n\n\nFurther Reading\n\nRead the motivation for MLOps from this blog.\nML Life Cycle CRISP-ML9(Q): blog, paper",
    "crumbs": [
      "Introduction",
      "00A: OSS for ML"
    ]
  }
]